{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Supervised_NLP_Challenges.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "zwfizmU_NvYn",
        "LRPqfbN5NvY3",
        "nglhdcZQNvZM",
        "flIo8_nANvZk",
        "6GgCCSLgNvZq",
        "xGUIuXelNvax",
        "y9jnxwxbNvbx",
        "j2-TK1huNvcA",
        "NxggG7Z0NvcO",
        "UMZzPuYXNvcs",
        "Goe2-T5aNvc3"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aarondelgiudice/thinkful_data_bootcamp/blob/master/Unit_4/lesson_4/Supervised_NLP_Challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_g8_uvjNvWE",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Processing Challenges\n",
        "## Part 1:\n",
        "Recall that the logistic regression model's best performance on the test set was 93%.\n",
        "See what you can do to improve performance.\n",
        "\n",
        "Suggested avenues of investigation include:\n",
        "- Other modeling techniques (SVM?)\n",
        "- making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc)\n",
        "- making sentence-level features (number of words, amount of punctuation)\n",
        "- including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc)\n",
        "\n",
        "Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 90%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yDmlzqbG6MBr",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from collections import Counter\n",
        "#nltk.download('gutenberg')\n",
        "#nltk.download('stopwords')\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yq5m1XSm6MB-",
        "run_control": {
          "frozen": false,
          "read_only": false
        }
      },
      "source": [
        "Supervised NLP requires a pre-labelled dataset for training and testing.\n",
        "In this case, we are going to try to predict whether a sentence comes from _Alice in Wonderland_ by Lewis Carroll or _Persuasion_ by Jane Austen.\n",
        "We can use any supervised model, as long as they allow categorical outcomes.\n",
        "In this case, we'll use Random Forests, SVM, and KNN.\n",
        "\n",
        "We'l generate features with  _BoW_, or _Bag of Words_.\n",
        "For each sentence, we count how many times each word appears.\n",
        "We will then use those counts as features.\n",
        "\n",
        "**Note**: Since processing all the text takes around ~5-10 minutes, in the cell below we are taking only the first tenth of each text. If you want to experiment, feel free to change the following code in the next cell:\n",
        "\n",
        "```python\n",
        "alice = text_cleaner(alice[:int(len(alice)/10)])\n",
        "persuasion = text_cleaner(persuasion[:int(len(persuasion)/10)])\n",
        "```\n",
        "to \n",
        "\n",
        "```python\n",
        "alice = text_cleaner(alice)\n",
        "persuasion = text_cleaner(persuasion)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8X44F6sQ6MCC",
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Utility function for standard text cleaning.\n",
        "def text_cleaner(text):\n",
        "    # Visual inspection identifies a form of punctuation spaCy does not\n",
        "    # recognize: the double dash '--'.  Better get rid of it now!\n",
        "    text = re.sub(r'--',' ',text)\n",
        "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
        "    text = re.sub(r'Chapter \\d+', '', text)\n",
        "    text = re.sub(r'VOLUME \\w+', '', text)\n",
        "    text = re.sub(r'CHAPTER \\w+', '', text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "    \n",
        "# Load and clean the data.\n",
        "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
        "alice = gutenberg.raw('carroll-alice.txt')\n",
        "emma = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "# The Chapter indicator is idiosyncratic\n",
        "#persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
        "#alice = re.sub(r'CHAPTER .*', '', alice)\n",
        "#emma = re.sub(r'VOLUME \\w+', '', emma)\n",
        "#emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
        "    \n",
        "alice = text_cleaner(alice[:int(len(alice)/10)])\n",
        "persuasion = text_cleaner(persuasion[:int(len(persuasion)/10)])\n",
        "# in order to get comparable length texts, take the first sixtieth of Emma\n",
        "emma = text_cleaner(emma[:int(len(emma)/60)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PijRJRmA6MCL",
        "colab": {}
      },
      "source": [
        "# Parse the cleaned novels. This can take a bit.\n",
        "nlp = spacy.load('en')\n",
        "alice_doc = nlp(alice)\n",
        "persuasion_doc = nlp(persuasion)\n",
        "emma_doc = nlp(emma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tHP34Nsr6MCa",
        "outputId": "321be98d-4f69-45a7-d311-b947a7da4c30",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Group into sentences.\n",
        "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
        "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
        "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
        "# view number of sentences\n",
        "print(len(alice_sents))\n",
        "print(len(persuasion_sents))\n",
        "print(len(emma_sents))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129\n",
            "315\n",
            "170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_E1OWVdNvXC",
        "colab_type": "code",
        "colab": {},
        "outputId": "82bdc332-4ab6-4adf-8949-9088b6543265"
      },
      "source": [
        "# For computational purposes, reduce length of each set of sentences to 500\n",
        "alice_sents = alice_sents[0:500]\n",
        "persuasion_sents = persuasion_sents[0:500]\n",
        "emma_sents = emma_sents[0:500]\n",
        "# view number of sentences\n",
        "print(len(alice_sents))\n",
        "print(len(persuasion_sents))\n",
        "print(len(emma_sents))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129\n",
            "315\n",
            "170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9EKlgcvNvXS",
        "colab_type": "code",
        "colab": {},
        "outputId": "ab678281-4937-48b3-951c-c1a6a817a97c"
      },
      "source": [
        "# Combine the sentences from the two novels into one data frame.\n",
        "sentences = pd.DataFrame(alice_sents + persuasion_sents + emma_sents)\n",
        "display(sentences.head())\n",
        "# view number of authors\n",
        "print(sentences.iloc[:, 1].unique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(.)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(Down, the, Rabbit, -, Hole, Alice, was, begin...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0        1\n",
              "0                                                (.)  Carroll\n",
              "1  (Down, the, Rabbit, -, Hole, Alice, was, begin...  Carroll\n",
              "2  (So, she, was, considering, in, her, own, mind...  Carroll\n",
              "3  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
              "4                                      (Oh, dear, !)  Carroll"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['Carroll' 'Austen']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lNhFLxl66MC1",
        "run_control": {
          "frozen": false,
          "read_only": false
        }
      },
      "source": [
        "Time to bag some words!  Since spaCy has already tokenized and labelled our data, we can move directly to recording how often various words occur.  We will exclude stopwords and punctuation.  In addition, in an attempt to keep our feature space from exploding, we will work with lemmas (root words) rather than the raw text terms, and we'll only use the 2000 most common words for each text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCy-LrSyNvXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bag_of_words(text):\n",
        "    '''\n",
        "    Utility function to create a list of the 2000 most common words.\n",
        "    '''\n",
        "    # Filter out punctuation and stop words.\n",
        "    allwords = [token.lemma_\n",
        "                for token in text\n",
        "                if not token.is_punct\n",
        "                and not token.is_stop]\n",
        "    \n",
        "    # Return the most common words.\n",
        "    return [item[0] for item in Counter(allwords).most_common(2000)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bcdqTvsa6MC7",
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "def bow_features(sentences, common_words):\n",
        "    '''\n",
        "    Creates a data frame with features for each word in our common word set.\n",
        "    Each value is the count of the times the word appears in each sentence.\n",
        "    '''\n",
        "    # Create column headers for sentences and author and initialize to 0\n",
        "    df = pd.DataFrame(columns=common_words)\n",
        "    df['text_sentence'] = sentences[0]\n",
        "    df['text_source'] = sentences[1]\n",
        "    df.loc[:, common_words] = 0\n",
        "    \n",
        "    # Process each row, counting the occurrence of words in each sentence.\n",
        "    for i, sentence in enumerate(df['text_sentence']):\n",
        "        \n",
        "        # Convert the sentence to lemmas, then filter out punctuation,\n",
        "        # stop words, and uncommon words.\n",
        "        words = [token.lemma_\n",
        "                 for token in sentence\n",
        "                 if (\n",
        "                     not token.is_punct\n",
        "                     and not token.is_stop\n",
        "                     and token.lemma_ in common_words\n",
        "                 )]\n",
        "        \n",
        "        # Populate the row with word counts.\n",
        "        for word in words:\n",
        "            df.loc[i, word] += 1\n",
        "        \n",
        "        # This counter is just to make sure the kernel didn't hang.\n",
        "        if i % 50 == 0:\n",
        "            print(\"Processing row {}\".format(i))\n",
        "            \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5gh8UC0NvX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up the bags.\n",
        "alicewords = bag_of_words(alice_doc)\n",
        "persuasionwords = bag_of_words(persuasion_doc)\n",
        "emmawords = bag_of_words(emma_doc)\n",
        "\n",
        "# Combine bags to create a set of unique words.\n",
        "common_words = set(alicewords + persuasionwords + emmawords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EdE9ERCl6MDD",
        "colab": {},
        "outputId": "93918a14-d2d2-4660-e093-ccafd4055155"
      },
      "source": [
        "# Create our data frame with features. This can take a while to run.\n",
        "word_counts = bow_features(sentences, common_words)\n",
        "word_counts.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing row 0\n",
            "Processing row 50\n",
            "Processing row 100\n",
            "Processing row 150\n",
            "Processing row 200\n",
            "Processing row 250\n",
            "Processing row 300\n",
            "Processing row 350\n",
            "Processing row 400\n",
            "Processing row 450\n",
            "Processing row 500\n",
            "Processing row 550\n",
            "Processing row 600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>equity</th>\n",
              "      <th>remnant</th>\n",
              "      <th>1760</th>\n",
              "      <th>1806</th>\n",
              "      <th>1789</th>\n",
              "      <th>encourage</th>\n",
              "      <th>requisition</th>\n",
              "      <th>suitable</th>\n",
              "      <th>favourite</th>\n",
              "      <th>nervous</th>\n",
              "      <th>...</th>\n",
              "      <th>personableness</th>\n",
              "      <th>confide</th>\n",
              "      <th>catch</th>\n",
              "      <th>insult</th>\n",
              "      <th>aware</th>\n",
              "      <th>flatter</th>\n",
              "      <th>presume</th>\n",
              "      <th>tire</th>\n",
              "      <th>text_sentence</th>\n",
              "      <th>text_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(.)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(Down, the, Rabbit, -, Hole, Alice, was, begin...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1800 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  equity remnant 1760 1806 1789 encourage requisition suitable favourite  \\\n",
              "0      0       0    0    0    0         0           0        0         0   \n",
              "1      0       0    0    0    0         0           0        0         0   \n",
              "2      0       0    0    0    0         0           0        0         0   \n",
              "3      0       0    0    0    0         0           0        0         0   \n",
              "4      0       0    0    0    0         0           0        0         0   \n",
              "\n",
              "  nervous     ...     personableness confide catch insult aware flatter  \\\n",
              "0       0     ...                  0       0     0      0     0       0   \n",
              "1       0     ...                  0       0     0      0     0       0   \n",
              "2       0     ...                  0       0     0      0     0       0   \n",
              "3       0     ...                  0       0     0      0     0       0   \n",
              "4       0     ...                  0       0     0      0     0       0   \n",
              "\n",
              "  presume tire                                      text_sentence text_source  \n",
              "0       0    0                                                (.)     Carroll  \n",
              "1       0    0  (Down, the, Rabbit, -, Hole, Alice, was, begin...     Carroll  \n",
              "2       0    0  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
              "3       0    0  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
              "4       0    0                                      (Oh, dear, !)     Carroll  \n",
              "\n",
              "[5 rows x 1800 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R3OH8pjQ6MDP",
        "run_control": {
          "frozen": false,
          "read_only": false
        }
      },
      "source": [
        "## Model Selection - BoW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bvXh9-xB6MDS",
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# define data, target\n",
        "target = word_counts['text_source']\n",
        "Data = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
        "# split training and testing data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    Data, target, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "zwfizmU_NvYn",
        "colab_type": "text"
      },
      "source": [
        "### Lasso Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xInM2qD_NvYq",
        "colab_type": "code",
        "colab": {},
        "outputId": "cfcb2792-0ccc-42ff-ef08-5eaf8eb09edb"
      },
      "source": [
        "# set penalty to 'l1'\n",
        "# set penalty to 'liblinear'\n",
        "lasso = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "lasso.fit(X_train, y_train)\n",
        "print('Training set score:', lasso.score(X_train, y_train))\n",
        "print('\\nTest set score:', lasso.score(X_test, y_test))\n",
        "cv_scores = cross_val_score(lasso, Data, target, cv=5)\n",
        "print('\\nCross validation:', cv_scores)\n",
        "print('\\nVariance:', np.var(cv_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.9124236252545825\n",
            "\n",
            "Test set score: 0.8699186991869918\n",
            "\n",
            "Cross validation: [0.87804878 0.83739837 0.85365854 0.8699187  0.8442623 ]\n",
            "\n",
            "Variance: 0.00023339902121696036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "LRPqfbN5NvY3",
        "colab_type": "text"
      },
      "source": [
        "### Ridge Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ra0SwSDjNvY6",
        "colab_type": "code",
        "colab": {},
        "outputId": "1487a95c-cd9b-446e-f881-0b740b94ccd6"
      },
      "source": [
        "# set penalty to 'l2'\n",
        "# set solver to 'lbfgs'\n",
        "ridge = LogisticRegression(penalty='l2', solver='lbfgs')\n",
        "ridge.fit(X_train, y_train)\n",
        "print('Training set score:', ridge.score(X_train, y_train))\n",
        "print('\\nTest set score:', ridge.score(X_test, y_test))\n",
        "cv_scores = cross_val_score(ridge, Data, target, cv=5)\n",
        "print('\\nCross validation:', cv_scores)\n",
        "print('\\nVariance:', np.var(cv_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.9531568228105907\n",
            "\n",
            "Test set score: 0.8780487804878049\n",
            "\n",
            "Cross validation: [0.87804878 0.83739837 0.90243902 0.8699187  0.87704918]\n",
            "\n",
            "Variance: 0.00043710162654028554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "nglhdcZQNvZM",
        "colab_type": "text"
      },
      "source": [
        "### Support Vector Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "liaX9QC-NvZQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "d90f861c-a3a6-4ab0-c13d-3b897e8d125a"
      },
      "source": [
        "# set gamma to 'scale'\n",
        "svc = SVC(gamma='scale')\n",
        "svc.fit(X_train, y_train)\n",
        "print('Training set score:', svc.score(X_train, y_train))\n",
        "print('\\nTest set score:', svc.score(X_test, y_test))\n",
        "cv_scores = cross_val_score(svc, Data, target, cv=5)\n",
        "print('\\nCross validation:', cv_scores)\n",
        "print('\\nVariance:', np.var(cv_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.8004073319755601\n",
            "\n",
            "Test set score: 0.7804878048780488\n",
            "\n",
            "Cross validation: [0.78861789 0.78861789 0.78861789 0.78861789 0.79508197]\n",
            "\n",
            "Variance: 6.6854949787822925e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flIo8_nANvZk",
        "colab_type": "text"
      },
      "source": [
        "## Feature Engineering\n",
        "**Sentence-level features**: sentence length, amount of punctuation, length of previous and next sentences, words repeated from one sentence to the next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GgCCSLgNvZq",
        "colab_type": "text"
      },
      "source": [
        "### Sentence-Level Feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRAXrlDSNvZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sentence length feature\n",
        "for i, sentence in enumerate(word_counts['text_sentence']):\n",
        "    word_counts.loc[i, 'sentenceLength'] = len(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "y_4ZjgquNvZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# amount of punctuation feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5_D0miieNvaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# length of previous and next sentences features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xF3ys3M2Nvab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# repeated words feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mlL45DjNval",
        "colab_type": "code",
        "colab": {},
        "outputId": "3329d513-98b0-4d46-9433-7a5639873416"
      },
      "source": [
        "word_counts.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>equity</th>\n",
              "      <th>remnant</th>\n",
              "      <th>1760</th>\n",
              "      <th>1806</th>\n",
              "      <th>1789</th>\n",
              "      <th>encourage</th>\n",
              "      <th>requisition</th>\n",
              "      <th>suitable</th>\n",
              "      <th>favourite</th>\n",
              "      <th>nervous</th>\n",
              "      <th>...</th>\n",
              "      <th>confide</th>\n",
              "      <th>catch</th>\n",
              "      <th>insult</th>\n",
              "      <th>aware</th>\n",
              "      <th>flatter</th>\n",
              "      <th>presume</th>\n",
              "      <th>tire</th>\n",
              "      <th>text_sentence</th>\n",
              "      <th>text_source</th>\n",
              "      <th>sentenceLength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(.)</td>\n",
              "      <td>Carroll</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(Down, the, Rabbit, -, Hole, Alice, was, begin...</td>\n",
              "      <td>Carroll</td>\n",
              "      <td>72.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
              "      <td>Carroll</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
              "      <td>Carroll</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1801 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  equity remnant 1760 1806 1789 encourage requisition suitable favourite  \\\n",
              "0      0       0    0    0    0         0           0        0         0   \n",
              "1      0       0    0    0    0         0           0        0         0   \n",
              "2      0       0    0    0    0         0           0        0         0   \n",
              "3      0       0    0    0    0         0           0        0         0   \n",
              "4      0       0    0    0    0         0           0        0         0   \n",
              "\n",
              "  nervous      ...       confide catch insult aware flatter presume tire  \\\n",
              "0       0      ...             0     0      0     0       0       0    0   \n",
              "1       0      ...             0     0      0     0       0       0    0   \n",
              "2       0      ...             0     0      0     0       0       0    0   \n",
              "3       0      ...             0     0      0     0       0       0    0   \n",
              "4       0      ...             0     0      0     0       0       0    0   \n",
              "\n",
              "                                       text_sentence text_source  \\\n",
              "0                                                (.)     Carroll   \n",
              "1  (Down, the, Rabbit, -, Hole, Alice, was, begin...     Carroll   \n",
              "2  (So, she, was, considering, in, her, own, mind...     Carroll   \n",
              "3  (There, was, nothing, so, VERY, remarkable, in...     Carroll   \n",
              "4                                      (Oh, dear, !)     Carroll   \n",
              "\n",
              "  sentenceLength  \n",
              "0            1.0  \n",
              "1           72.0  \n",
              "2           63.0  \n",
              "3           30.0  \n",
              "4            3.0  \n",
              "\n",
              "[5 rows x 1801 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGUIuXelNvax",
        "colab_type": "text"
      },
      "source": [
        "### TF-IDF Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6oi-hOrNva2",
        "colab_type": "code",
        "colab": {},
        "outputId": "7991d1e4-f52c-4799-98e5-d02cafd593ed"
      },
      "source": [
        "# Create vectorizer model in order to get tf-idf for each sentence\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_df=0.5, # drop words that occur in more than half the paragraphs\n",
        "    min_df=2, # only use words that appear at least twice\n",
        "    stop_words='english', \n",
        "    lowercase=False, #c onvert everything to lower case\n",
        "    use_idf=True,# use inverse document frequencies in our weighting\n",
        "    norm=u'l2', # Applies correction factor so long paragraphs and short paragraphs are equally weighted\n",
        "    smooth_idf=True # Prevents divide-by-zero errors\n",
        "    )\n",
        "\n",
        "# Convert (text_sentence) from spacy object to string\n",
        "sentence_list = word_counts['text_sentence'].astype(str)\n",
        "print(type(sentence_list))\n",
        "\n",
        "# Pass pandas series to our vectorizer model\n",
        "text_tfidf = vectorizer.fit_transform(sentence_list)\n",
        "print(type(text_tfidf))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xLmMrmfNvbG",
        "colab_type": "code",
        "colab": {},
        "outputId": "f0c7d6fd-3df5-47f9-bf35-06219b4ded3d"
      },
      "source": [
        "# get list of features\n",
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "# shape\n",
        "n = text_tfidf.shape[0]\n",
        "\n",
        "# create list of dictionaries per paragraph\n",
        "tfidf_bysent = [{} for _ in range(0,n)]\n",
        "\n",
        "# for each sentence, lists the feature words and their tf-idf scores\n",
        "for i, j in zip(*text_tfidf.nonzero()):\n",
        "    tfidf_bysent[i][terms[j]] = text_tfidf[i, j]\n",
        "\n",
        "# view dictionary\n",
        "display(tfidf_bysent[1])\n",
        "print(type(tfidf_bysent))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'Down': 0.2199070353564796,\n",
              " 'Rabbit': 0.1903594676871988,\n",
              " 'Alice': 0.28386045468497895,\n",
              " 'beginning': 0.2051332515218392,\n",
              " 'tired': 0.23038920180314254,\n",
              " 'sister': 0.3807189353743976,\n",
              " 'having': 0.1693951347938729,\n",
              " 'twice': 0.23038920180314254,\n",
              " 'book': 0.3893021701503525,\n",
              " 'reading': 0.23038920180314254,\n",
              " 'pictures': 0.4398140707129592,\n",
              " 'use': 0.19465108507517626,\n",
              " 'thought': 0.15462135095923252,\n",
              " 'conversation': 0.23038920180314254}"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "34lRfo23NvbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dataframe for this feature set\n",
        "tfidf_df = pd.DataFrame(columns=terms)\n",
        "tfidf_df['text_sentence'] = word_counts['text_sentence']\n",
        "tfidf_df['text_source'] = word_counts['text_source']\n",
        "tfidf_df.loc[:, terms] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "DdJFktLnNvbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counter = 0\n",
        "for i in tfidf_bysent:\n",
        "    for k, v in i.items():\n",
        "        tfidf_df.loc[counter, k] = v\n",
        "    counter = counter + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9jnxwxbNvbx",
        "colab_type": "text"
      },
      "source": [
        "### Concat feature Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jXn78h4Nvb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.concat([word_counts, tfidf_df.drop('text_source', axis=1)], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2-TK1huNvcA",
        "colab_type": "text"
      },
      "source": [
        "## Model Selection - TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "BJF-waQ5NvcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define data, target\n",
        "target = word_counts['text_source']\n",
        "Data = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
        "# split training and testing data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    Data, target, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "NxggG7Z0NvcO",
        "colab_type": "text"
      },
      "source": [
        "### Lasso Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "nhlKs0jVNvcf",
        "colab_type": "code",
        "colab": {},
        "outputId": "f6d1b177-cc6b-4fd1-ef89-f1cbee71ba54"
      },
      "source": [
        "# set penalty to 'l1'\n",
        "# set penalty to 'liblinear'\n",
        "lasso = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "lasso.fit(X_train, y_train)\n",
        "print('Training set score:', lasso.score(X_train, y_train))\n",
        "print('\\nTest set score:', lasso.score(X_test, y_test))\n",
        "cv_scores = cross_val_score(lasso, Data, target, cv=5)\n",
        "print('\\nCross validation:', cv_scores)\n",
        "print('\\nVariance:', np.var(cv_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.9205702647657841\n",
            "\n",
            "Test set score: 0.8780487804878049\n",
            "\n",
            "Cross validation: [0.87804878 0.85365854 0.8699187  0.86178862 0.85245902]\n",
            "\n",
            "Variance: 9.480485206068642e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "UMZzPuYXNvcs",
        "colab_type": "text"
      },
      "source": [
        "### Ridge Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "qYRou5y5Nvcu",
        "colab_type": "code",
        "colab": {},
        "outputId": "506c44eb-13a9-4779-ce83-864f2073d6ce"
      },
      "source": [
        "# set penalty to 'l2'\n",
        "# set solver to 'lbfgs'\n",
        "ridge = LogisticRegression(penalty='l2', solver='lbfgs')\n",
        "ridge.fit(X_train, y_train)\n",
        "print('Training set score:', ridge.score(X_train, y_train))\n",
        "print('\\nTest set score:', ridge.score(X_test, y_test))\n",
        "cv_scores = cross_val_score(ridge, Data, target, cv=5)\n",
        "print('\\nCross validation:', cv_scores)\n",
        "print('\\nVariance:', np.var(cv_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aaron\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.9531568228105907\n",
            "\n",
            "Test set score: 0.8699186991869918\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aaron\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "C:\\Users\\Aaron\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "C:\\Users\\Aaron\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "C:\\Users\\Aaron\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cross validation: [0.87804878 0.83739837 0.90243902 0.86178862 0.87704918]\n",
            "\n",
            "Variance: 0.00045760291112130393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Goe2-T5aNvc3",
        "colab_type": "text"
      },
      "source": [
        "### Support Vector Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Ji4j20MmNvdJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "d96909ba-c9eb-4752-d98e-97c750a19214"
      },
      "source": [
        "# set gamma to 'scale'\n",
        "svc = SVC(gamma='scale')\n",
        "svc.fit(X_train, y_train)\n",
        "print('Training set score:', svc.score(X_train, y_train))\n",
        "print('\\nTest set score:', svc.score(X_test, y_test))\n",
        "cv_scores = cross_val_score(svc, Data, target, cv=5)\n",
        "print('\\nCross validation:', cv_scores)\n",
        "print('\\nVariance:', np.var(cv_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.7922606924643585\n",
            "\n",
            "Test set score: 0.7804878048780488\n",
            "\n",
            "Cross validation: [0.78861789 0.78861789 0.78861789 0.78861789 0.79508197]\n",
            "\n",
            "Variance: 6.6854949787822925e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HjXQfitY6MFU",
        "run_control": {
          "frozen": false,
          "read_only": false
        }
      },
      "source": [
        "# Part 1: Conclusion\n",
        "The best performing model was the Lasso Logistic Regression.\n",
        "Our SVC underperformed and the Ridge Regression never converged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn5kCX6yNvdg",
        "colab_type": "code",
        "colab": {},
        "outputId": "5cafca02-2477-4f75-d290-d15b01f11081"
      },
      "source": [
        "pd.crosstab(y_test, lasso.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>Austen</th>\n",
              "      <th>Carroll</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text_source</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Austen</th>\n",
              "      <td>96</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Carroll</th>\n",
              "      <td>15</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0        Austen  Carroll\n",
              "text_source                 \n",
              "Austen           96        0\n",
              "Carroll          15       12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q99WdTmSNvdq",
        "colab_type": "text"
      },
      "source": [
        "With the new feature set we saw a marginal increase in our model performance, but a slight increase in variance.\n",
        "Our model was not able to improve on the original model's score of 93%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaIdfTz1Nvds",
        "colab_type": "text"
      },
      "source": [
        "# Part 2:\n",
        "Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work.\n",
        "This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "vFQwqWzSNvdv",
        "colab_type": "code",
        "colab": {},
        "outputId": "0e772519-96d9-4138-bcab-b258778787ca"
      },
      "source": [
        "print(gutenberg.fileids())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnDnjfxlNvd3",
        "colab_type": "text"
      },
      "source": [
        "For our new model we'll use Shakepeare's Julius Caesar text.\n",
        "The classification power of our model should be much stronger as the style of language used by Shakespeare is very different from other authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5Yo0brjNvd5",
        "colab_type": "code",
        "colab": {},
        "outputId": "536b7baa-5853-413f-8f13-516aded6c4c3"
      },
      "source": [
        "#specify path for new text\n",
        "PATH = 'shakespeare-caesar.txt'\n",
        "# Load and clean the data.\n",
        "alice = gutenberg.raw('carroll-alice.txt')\n",
        "alice = text_cleaner(alice[:int(len(alice)/10)])\n",
        "new_text = gutenberg.raw(PATH)\n",
        "new_text = text_cleaner(new_text[:int(len(alice))])\n",
        "print(new_text[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actus Primus. Scoena Prima. Enter Flauius, Murellus, and certaine Commoners ouer the Stage. Flauius.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTcUwXbjNveL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parse the cleaned novels. This can take a bit.\n",
        "#nlp = spacy.load('en')\n",
        "#alice_doc = nlp(alice)\n",
        "new_text_doc = nlp(new_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vEhtzD1NveS",
        "colab_type": "code",
        "colab": {},
        "outputId": "9758aebf-cc72-4e05-9dd4-d5aa256bf355"
      },
      "source": [
        "Author = 'Shakespeare'\n",
        "# Group into sentences.\n",
        "#alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
        "new_text_sents = [[sent, Author] for sent in new_text_doc.sents]\n",
        "# view number of sentences\n",
        "print(len(alice_sents))\n",
        "print(len(new_text_sents))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129\n",
            "271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGLE36waNveb",
        "colab_type": "code",
        "colab": {},
        "outputId": "a12ad122-42cd-40b7-f7e2-cde32086a6a9"
      },
      "source": [
        "# For computational purposes, reduce length of each set of sentences to 500\n",
        "#alice_sents = alice_sents[0:500]\n",
        "new_text_sents = new_text_sents[0:500]\n",
        "# view number of sentences\n",
        "print(len(alice_sents))\n",
        "print(len(new_text_sents))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129\n",
            "271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpNRh1FcNve0",
        "colab_type": "code",
        "colab": {},
        "outputId": "f81b869c-964a-4460-f0ac-18d2f6ad41f2"
      },
      "source": [
        "# Combine the sentences from the two novels into one data frame.\n",
        "sentences = pd.DataFrame(alice_sents + new_text_sents)\n",
        "display(sentences.head())\n",
        "# view number of authors\n",
        "print(sentences.iloc[:, 1].unique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(.)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(Down, the, Rabbit, -, Hole, Alice, was, begin...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0        1\n",
              "0                                                (.)  Carroll\n",
              "1  (Down, the, Rabbit, -, Hole, Alice, was, begin...  Carroll\n",
              "2  (So, she, was, considering, in, her, own, mind...  Carroll\n",
              "3  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
              "4                                      (Oh, dear, !)  Carroll"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['Carroll' 'Shakespeare']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUiYSq7PNvfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up the bags.\n",
        "#alicewords = bag_of_words(alice_doc)\n",
        "new_text_words = bag_of_words(new_text_doc)\n",
        "# Combine bags to create a set of unique words.\n",
        "common_words = set(alicewords + new_text_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaJ7Mr3kNvfL",
        "colab_type": "code",
        "colab": {},
        "outputId": "72321218-f681-4df0-f0d4-aed27e38629d"
      },
      "source": [
        "# Create our data frame with features. This can take a while to run.\n",
        "word_counts = bow_features(sentences, common_words)\n",
        "word_counts.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing row 0\n",
            "Processing row 50\n",
            "Processing row 100\n",
            "Processing row 150\n",
            "Processing row 200\n",
            "Processing row 250\n",
            "Processing row 300\n",
            "Processing row 350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>whereof</th>\n",
              "      <th>white</th>\n",
              "      <th>desire</th>\n",
              "      <th>paper</th>\n",
              "      <th>hot</th>\n",
              "      <th>vnder</th>\n",
              "      <th>Brutus</th>\n",
              "      <th>Loue</th>\n",
              "      <th>dip</th>\n",
              "      <th>shoulder</th>\n",
              "      <th>...</th>\n",
              "      <th>flourish</th>\n",
              "      <th>catch</th>\n",
              "      <th>high</th>\n",
              "      <th>proper</th>\n",
              "      <th>garden</th>\n",
              "      <th>brighten</th>\n",
              "      <th>lovely</th>\n",
              "      <th>poor</th>\n",
              "      <th>text_sentence</th>\n",
              "      <th>text_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(.)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(Down, the, Rabbit, -, Hole, Alice, was, begin...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(Oh, dear, !)</td>\n",
              "      <td>Carroll</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1067 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  whereof white desire paper hot vnder Brutus Loue dip shoulder     ...      \\\n",
              "0       0     0      0     0   0     0      0    0   0        0     ...       \n",
              "1       0     0      0     0   0     0      0    0   0        0     ...       \n",
              "2       0     0      0     0   1     0      0    0   0        0     ...       \n",
              "3       0     0      0     0   0     0      0    0   0        0     ...       \n",
              "4       0     0      0     0   0     0      0    0   0        0     ...       \n",
              "\n",
              "  flourish catch high proper garden brighten lovely poor  \\\n",
              "0        0     0    0      0      0        0      0    0   \n",
              "1        0     0    0      0      0        0      0    0   \n",
              "2        0     0    0      0      0        0      0    0   \n",
              "3        0     0    0      0      0        0      0    0   \n",
              "4        0     0    0      0      0        0      0    0   \n",
              "\n",
              "                                       text_sentence text_source  \n",
              "0                                                (.)     Carroll  \n",
              "1  (Down, the, Rabbit, -, Hole, Alice, was, begin...     Carroll  \n",
              "2  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
              "3  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
              "4                                      (Oh, dear, !)     Carroll  \n",
              "\n",
              "[5 rows x 1067 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-DIaMBfNvfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sentence length feature\n",
        "for i, sentence in enumerate(word_counts['text_sentence']):\n",
        "    word_counts.loc[i, 'sentenceLength'] = len(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5J1HW5zNvfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert (text_sentence) from spacy object to string\n",
        "sentence_list = word_counts['text_sentence'].astype(str)\n",
        "# Pass pandas series to our vectorizer model\n",
        "text_tfidf = vectorizer.fit_transform(sentence_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8paZQ-tNvfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get list of features\n",
        "terms = vectorizer.get_feature_names()\n",
        "# shape\n",
        "n = text_tfidf.shape[0]\n",
        "# create list of dictionaries per paragraph\n",
        "tfidf_bysent = [{} for _ in range(0,n)]\n",
        "# for each sentence, lists the feature words and their tf-idf scores\n",
        "for i, j in zip(*text_tfidf.nonzero()):\n",
        "    tfidf_bysent[i][terms[j]] = text_tfidf[i, j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylaH7PBmNvfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create dataframe for this feature set\n",
        "tfidf_df = pd.DataFrame(columns=terms)\n",
        "tfidf_df['text_sentence'] = word_counts['text_sentence']\n",
        "tfidf_df['text_source'] = word_counts['text_source']\n",
        "tfidf_df.loc[:, terms] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kojgHlDmNvf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counter = 0\n",
        "for i in tfidf_bysent:\n",
        "    for k, v in i.items():\n",
        "        tfidf_df.loc[counter, k] = v\n",
        "    counter = counter + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUdphHOnNvgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_counts = pd.concat(\n",
        "    [word_counts, tfidf_df.drop('text_source', axis=1)], axis=1\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os7_TnNtNvgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define data, target\n",
        "target = word_counts['text_source']\n",
        "Data = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
        "# split training and testing data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    Data, target, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng5mSAg4NvgM",
        "colab_type": "code",
        "colab": {},
        "outputId": "9a5c915b-a0c7-494b-c023-d76b5c77d2ce"
      },
      "source": [
        "# set penalty to 'l1'\n",
        "# set penalty to 'liblinear'\n",
        "lasso = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "lasso.fit(X_train, y_train)\n",
        "print('Training set score:', lasso.score(X_train, y_train))\n",
        "print('\\nTest set score:', lasso.score(X_test, y_test))\n",
        "cv_scores = cross_val_score(lasso, Data, target, cv=5)\n",
        "print('\\nCross validation:', cv_scores)\n",
        "print('\\nVariance:', np.var(cv_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.915625\n",
            "\n",
            "Test set score: 0.7875\n",
            "\n",
            "Cross validation: [0.87654321 0.8125     0.9        0.8625     0.78481013]\n",
            "\n",
            "Variance: 0.0017959030603215118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhVgZXSvNvgT",
        "colab_type": "code",
        "colab": {},
        "outputId": "8efaca37-0740-4b59-c0c4-52ea0044a220"
      },
      "source": [
        "pd.crosstab(y_test, lasso.predict(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>Carroll</th>\n",
              "      <th>Shakespeare</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text_source</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Carroll</th>\n",
              "      <td>17</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Shakespeare</th>\n",
              "      <td>1</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0        Carroll  Shakespeare\n",
              "text_source                      \n",
              "Carroll           17           16\n",
              "Shakespeare        1           46"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSriux4jNvgd",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Conclusion\n",
        "Our model did not perform as well as the Carroll-Austen model.\n",
        "This is surprising as the style of language is so very different.\n",
        "It could have something to do with the structure of Shakespeare's play compared to Carroll's novel.\n",
        "Our cleaning function wasn't as effective with Shakespeare ('scoena Prima' vs 'chapter 1')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHaxwzmUNvgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}