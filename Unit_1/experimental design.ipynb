{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Design\n",
    "\n",
    "Unit 1 / Lesson 4\n",
    "\n",
    "Designing an experiment is only way to get bespoke data, or data that is perfectly suited to the quetions at hand.\n",
    "Unlike working with existing datasets, properly designed experiments will have every variable you need, measured the way you think best, and collected in a context that emphasizes the features you want to understand.\n",
    "\n",
    "Experimental design is a powerful tool when implemented correctly. In this lesson we'll go over A/B testing, how to evaluate a good experiment, how to write a research proposal, and analytical techniques to use when an experiment yields non-normal data.\n",
    "\n",
    "# A/B Tests\n",
    "\n",
    "Unit 1 / Lesson 1 / Project 1\n",
    "\n",
    "There are many possible [experimental designs](https://cirt.gcu.edu/research/developmentresources/research_ready/experimental/design_types), but __A/B testing__ is the most common.\n",
    "__A/B testing__ is used to indentify whether one version object of interest (product, marketing campaign, email text, etc.) is better at producing a desired outcome than another.\n",
    "\n",
    "The components of an __A/B test__ are:\n",
    "- __Two versions__ of something to compare. Typically one version is the \"control\" and the other is the \"test version\". The control version is typically already in use, and the test version has some changes or new features, called \"the treatment\". However, it's possible to have two different test versions to compare to each other\n",
    "- A __sample__ divided into two groups. Each group should be selected so that is similar to the population you want to understand. The groups should be similar to one another, so any differences can be attributed to to which version you're viewing--A or B--and not attributable to some other factor. The split between group A and B should be as random as possible.\n",
    "- A __hypothesis__ is what you expect to happen. For example, we expect HTML email to have a higher conversion rate than plain text email.\n",
    "- __Outcomes__ of interest, or what you expect will change as a result of using version A rather than version B, and how you will measure that change. To do this, you must decide on a _key metric_ which will capture the effect of your change and reflect the motivations for the test.\n",
    "- __Other measure variables__ that include information about the two groups and secondary outcomes of the test. This can be used to ensure the groups are similar and to see if outcomes change depending on which version--A or B--we're testing.\n",
    "\n",
    "\n",
    "### Getting a Good Sample\n",
    "\n",
    "A good sample is key to a successful __A/B test__.\n",
    "It's important to make sure the two groups--A and B--are as comparable as possible when actually designing your experiment.\n",
    "The only difference should be the treatment the groups receive.\n",
    "\n",
    "If you have, for example, a constantly occuring randomizable test where the thing you're measuring is happening continuously is easy to differentiate occurances, getting a good sample is pretty easy.\n",
    "Marketing emails are usually like this; thousands of emails are sent out every moment and we can easily randomize between versions A and B.\n",
    "\n",
    "Hower plenty of things can make this more complex.\n",
    "The most common challenge is a test that either has to be \"all on\" or \"all off\".\n",
    "This could be something like whether playing music in a shop makes customers stay longer.\n",
    "You really can't have different groups of customers, one listening to music and one not, in the same shop.\n",
    "\n",
    "In that case, randomization because a necessity. You want to consider factors like seasonality, day of the week, time of day, weather. Perhaps behavior is different later in the day, on weekends, or in the summer?\n",
    "\n",
    "### Keys to Key Metrics\n",
    "\n",
    "The other key to success to determining what you want to measure.\n",
    "You should monitor multiple metrics during most tests, but you should have one _key_ metric that will be your determining factor of whether or not your test was a success.\n",
    "\n",
    "A good key metric should be as closely related to your goal as possible.\n",
    "Measuring clicks can be valuable, but not necessarily if you're interested in maximizing revenue.\n",
    "Then you would want to measure purchases and sales.\n",
    "This will prevent getting a test result that affects an intermediate step, not our actual goal.\n",
    "\n",
    "You'll also want to make sure your key metric can be reliably measured. Experiments can be ruined by measurement error.\n",
    "Ideally, a reliable metric is passively measured.\n",
    "That is, it does not require an addition step, such as engagement from subjects or users.\n",
    "You also want to avoid self-reported data as that can lead to potential bias.\n",
    "\n",
    "You want to make sure to capture the complete effect of your change.\n",
    "In many cases, time becomes the most important factor.\n",
    "For example. an email campaign doesn't experience an immediate increase in sign ups, but customers may convert over time.\n",
    "Try to include a time window to your key metric that is reasonable and consistent across all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
