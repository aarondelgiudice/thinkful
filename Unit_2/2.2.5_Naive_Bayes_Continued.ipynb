{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Continued\n",
    "Unit 2 / Lesson 2 / Assignment 5\n",
    "\n",
    "We’ve introduced the core concept of a __Naive Bayes Classifier__, but there are still some details to sort out.\n",
    "In this section we’ll cover a bit of the decision process that goes into running the model, ways to improve performance, and some of the risks and downsides of using __Naive Bayes__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Naive Bayes\n",
    "\n",
    "When actually running a __Naive Bayes classifier__, you will have to make one more _assumption_.\n",
    "That _assumption_ is around the distribution of P(xi|y).\n",
    "\n",
    "There are three main __classifiers__: __Bernoulli__, __Multinomial__, and __Gaussian Naive Bayes__.\n",
    "Each __classifier__ assumes that the distribution of the conditional (the aforementioned P(xi|y)) is the given distribution.\n",
    "\n",
    "Now these distributions have limitations.\n",
    "A _binomial_ only takes two possible values, a _multinomial_ has discrete outcomes, and a _Gaussian (also known as \"normal\")_ takes values along the continuous normal distribution.\n",
    "\n",
    "What this means is that choosing which kind of __classifier__ you want to use _depends on the distribution of your outcome variable_.\n",
    "Choose the distribution that best fits your data.\n",
    "\n",
    "It should be pretty obvious, for instance, if it is best modeled using __Bernoulli__ (the variable will be binary) and so on.\n",
    "\n",
    "If you’re interested in reading further about these types of __Bayes classifiers__, you can check out the [scikit-learn documentation](https://scikit-learn.org/stable/modules/naive_bayes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Performance\n",
    "\n",
    "In running this kind of __classifier__, there are many things you can do to improve the performance of your model.\n",
    "\n",
    "The first and most important thing will be, as is often the case, _feature engineering_.\n",
    "This is particularly true in text based problems (which we’ll cover at greater length in the NLP section later in the course).\n",
    "Here it is largely up to the creativity and knowledge of the person building the model to draw out the right features.\n",
    "\n",
    "In __Naive Bayes__, feature selection can also be important as features are equally weighted and heavily correlated features can lead to doubling the impact of what is essentially a single feature.\n",
    "Remember, you’re making the assumption that every pair of variables is independent of each other.\n",
    "The more removed from that assumption reality is, the more problems you may run into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsides of Naive Bayes\n",
    "\n",
    "The first and most obvious downside of __Naive Bayes__ is that assumption of independence.\n",
    "That is a double edged sword as not only is it a condition you’ll often fail to have (even when the model works well), but it also means that any time two variables affect the outcome most in concert your model will fail to see it.\n",
    "This kind of effect is called _interaction_, and occurs when any two features create a different effect when they both have a specific value than they would count as independent occurrences.\n",
    "\n",
    "_In __Naive Bayes__ any such interaction is lost_.\n",
    "\n",
    "__Naive Bayes__ can only predict the outcome of categories it has seen before.\n",
    "This applies both to the outcome and the inputs.\n",
    "If you have new x-values in test, the model will default to ignoring that specific outcome.\n",
    "Like all __classifiers__ it cannot predict a class it hasn’t seen.\n",
    "The way __Naive Bayes__ handles partial data, however, does have the benefit of being indifferent to missing datapoints.\n",
    "Those missing datapoints simply get ignored, drawing what information it can from the other variables of that observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
