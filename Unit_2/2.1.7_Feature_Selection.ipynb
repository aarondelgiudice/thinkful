{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "Unit 2 / Lesson 1 / Assignment 7\n",
    "\n",
    "__Feature selection__ is like handing out roses on The Bachelor.\n",
    "We want to keep the features that have the strongest connection to the outcome, while also prioritizing features that bring something unique to the table.\n",
    "Unlike _The Bachelor_, our goal isn't to narrow the options down to only one ideal featurette, but to settle on the set of features that is relatively straightforward to understand, is predictively powerful, minimizes overfitting, and is relatively computationally efficient.\n",
    "__Feature selection__ is a balancing act between explanatory power and model parsimony.\n",
    "Fortunately, many __feature selection__ algorithms are available to help data scientists optimize their feature sets.\n",
    "\n",
    "The one thing all __feature selection__ algorithms have in common is that they work better when data is separated into a training set and a test set, and feature selection is run on the training set.\n",
    "\n",
    "__Feature selection__ algorithms fall into three broad groups, _filter methods_, _wrapper methods_, and _embedded methods_.\n",
    "\n",
    "\n",
    "### Filter methods\n",
    "\n",
    "__Filter methods__ evaluate each feature separately and assign it a \"score\" that is used to rank the features, with scores above a certain cutoff point being retained or discarded.\n",
    "The feature may be evaluated independently of the outcome, or in combination with it.\n",
    "Variance thresholds, where only features with a variance above a certain cutoff are retained, are an example of independently evaluating features.\n",
    "The correlation of each feature with the outcome can also be used as a __filter method__.\n",
    "\n",
    "__Filter methods__ are good at selecting relevant features that are likely to be related to the outcome.\n",
    "They are computationally simple and straightforward, but likely to produce lists of redundant features since inter-feature relationships are not considered.\n",
    "Because they're \"cheap\" to run, you might use __filter methods__ as a first pass at reducing features before applying more computationally demanding algorithms like _wrapper methods_.\n",
    "\n",
    "\n",
    "### Wrapper methods\n",
    "\n",
    "__Wrapper methods__ select sets of features.\n",
    "Different sets are constructed, evaluated in terms of their predictive power in a model, and performance is compared to the performance of other sets.\n",
    "__Wrapper methods__ differ in terms of how the sets of features are constructed.\n",
    "Two such feature construction methods are \"forward passes\" and \"backward passes\".\n",
    "In _forward passes_, the algorithm begins with no features and adds features one-by-one, always adding the feature that results in the highest increase in predictive power and stopping at some predetermined threshold.\n",
    "In _backward passes_, the algorithm begins with all features and drops features one-by-one, always dropping the feature with the least predictive power and stopping at some predetermined threshold.\n",
    "_Forward_ and _backward pass_ methods are considered \"greedy\" because once a feature is added (forward) or removed (backward) it is never again evaluated for the model.\n",
    "\n",
    "__Wrapper methods__ are good at selecting useful sets of features that effectively predict the outcome.\n",
    "For larger sets of features, however, __wrapper methods__ can be highly computationally intensive and are more vulnerable to overfitting than filter methods.\n",
    "\n",
    "\n",
    "### Embedded methods\n",
    "\n",
    "__Embedded methods__ also select sets of features, but do so as an intrinsic part of the _fitting method_ for the particular type of model you're using.\n",
    "This may involve _regularization_, where a \"complexity penalty\" is added to goodness-of-fit measures typically used to assess the predictive power of a model.\n",
    "__Embedded methods__ provide the benefits of _wrapper methods_ but are less computationally intensive.\n",
    "Different types of models will use different __embedded methods__.\n",
    "\n",
    "\n",
    "For a deep dive into the world of feature selection algorithms, check out [An Introduction to Variable and Feature Selection by Isabelle Guyon and Andre Elisseef](http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf), in the Journal of Machine Learning Research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
