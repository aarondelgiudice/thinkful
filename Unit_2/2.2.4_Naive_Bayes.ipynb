{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "Unit 2 / Lesson 2 / Assignment 4\n",
    "\n",
    "The time has come to learn your first real model: __Naive Bayes__.\n",
    "The reason we're introducing this model first is because, actually, we already introduced everything you need to know a while ago.\n",
    "You should be familiar with _Bayes Rule_, which we covered in the fundamentals course (if you've forgotten don't worry, we'll cover it again briefly here).\n",
    "__Naive Bayes__ is simply modeling and prediction based around this theorem.\n",
    "\n",
    "Let's approach this by thinking about __Naive Bayes__ in terms of the two words in its name: _Naive_ and _Bayes_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes\n",
    "\n",
    "Let's discuss __Bayes__ first, since that's the core of the model.\n",
    "__Bayes Theorem__ covers the probabilistic relationship between multiple variables, and specifically allows us to define one conditional in terms of the underlying probabilities and the inverse condition.\n",
    "\n",
    "Specifically, it can be defined as:\n",
    "$$P(y|x) = P(y)P(x|y)/P(x)$$\n",
    "\n",
    "In English this reads as \"the probability of y given x equals the probability of y times the probability of x given y divided by the probability of x.\"\n",
    "\n",
    "This theorem can be extended to when x is a vector (containing the multiple x variables used as inputs for the model) to:\n",
    "$$P(y|x_1,...,x_n) = P(y)P(x_1,...,x_n|y)/P(x_1,...,x_n)$$\n",
    "\n",
    "This explains the relationship of an outcome to a vector of conditions rather than to a single other event.\n",
    "Recall that this can be read as the probability of y, in the case of our model the categorical outcome weâ€™re interested in, given a set of observations is equal to the probability of that set of observations given y divided by the probability of that set of outcomes.\n",
    "\n",
    "We'll return to this probability later to define our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive\n",
    "\n",
    "The other part of __Naive Bayes__ is of course __Naive__.\n",
    "In this setting __Naive__ refers to the assumption that any pair of variables in the conditional vector (the x variables) are _independent_ from each other.\n",
    "\n",
    "That independence does something really important to the vectorized __Bayes Rule__ equation (the second one from above). It allows us to break that large $P(x_1,...,x_n|y)$ into the product of each individual condition.\n",
    "Written out it would be something like:\n",
    "$$P(y|x_1,...,x_n) = P(y)*P(x_1|y)*...*P(x_n|y)/P(x_1,...,x_n)$$\n",
    "\n",
    "We can even simplify further because for any observation we are attempting to predict, the x-vector will be constant, so that part of the probability simplifies out leaving:\n",
    "$$P(y|x_1,...,x_n) \\approx P(y)*P(x_1|y)*...*P(x_n|y)$$\n",
    "\n",
    "So\n",
    "$$\\hat{y} = argmax_y(P(y)\\prod_{i=1}^nP(x_i|y))$$\n",
    "\n",
    "This states that our estimator of y is the maximum over y of the $P(y)*\\prod_{i=1}^nP(x_i|y)$.\n",
    "\n",
    "This is the basis for __Naive Bayes__ as a model.\n",
    "As you can tell from this formula, __Naive Bayes__ returns the y value that maximizes the following argument.\n",
    "This means it _returns a single value or category_.\n",
    "\n",
    "Returning to the fact that our estimate is the y that maximizes the argument, this is because __Naive Bayes__ is used as a _classifier_.\n",
    "We are interested in which y value is most likely to have given the observed set of x variables based on their _Bayesian probabilities_.\n",
    "There are ways to jigger the rule into returning probabilities, but these are generally NOT very accurate and not to be used.\n",
    "It is said that this is a good classifier but not a good estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple is Sometimes Better\n",
    "\n",
    "Now, we made some huge assumptions here to get to this predictor.\n",
    "The assumption of independence between each pair is _hugely important_ but _rarely totally accurate_.\n",
    "The columns of your data frame are typically not independent of each other.\n",
    "\n",
    "However, __Naive Bayes__ is still used in the real world.\n",
    "\n",
    "There are a few different reasons for that.\n",
    "Firstly, it is delightfully simple.\n",
    "It is easy to understand both how it operates and what it's doing.\n",
    "More importantly it is incredibly fast.\n",
    "That speed can occasionally be very useful from a practical perspective.\n",
    "It also relies on probabilities, which are based on counts, so you can actually train the classifier with more data than could fit into memory at one time (and scikit-learn even has an option to do this).\n",
    "That count reliance also contributes to its computational simplicity.\n",
    "\n",
    "There are also specific situations where __Naive Bayes__ has been known to perform reasonably well.\n",
    "This is most common in _sentiment classification_, a branch of machine learning that is designed to focus on trying to classify textual samples according to sentiment.\n",
    "Practically it is very good for spam filtering or telling if comments are positive or negative.\n",
    "\n",
    "We'll make a Bayesian spam filter later in this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
