{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7.1e_contextual_embeddings",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "MpK1-yGdIdCM",
        "Y-3XudGtN6SC",
        "465Ev8JTYS3W",
        "jK3t80pPRzTO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aarondelgiudice/thinkful_data_bootcamp/blob/master/unit_7/capstone/7_1e_contextual_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YZKWSnJ17Sp",
        "colab_type": "text"
      },
      "source": [
        "# Lyric Contextual Embeddings\n",
        "- Embedding Vectors\n",
        "- Model Embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnIzD9bFYy1c",
        "colab_type": "text"
      },
      "source": [
        "For our final approach we'll use contextual embeddings to model our lyric data. We'll use ELMo to generate 1024 dimensional embedding vectors for each song. We can then use those vectors as features to model our target variable.\n",
        "\n",
        "Rather than using the cleaned *and* parsed lyric data, we'll use only the cleaned lyric data. After parsing, some of the lyrics are too short to generate meaningful vectors. Longer texts--such as chapters of a novel or newspaper articles--are better suited for this approach.\n",
        "\n",
        "Once we've generated our embedding vectors, we'll model them both alone and in addition to Spotify's audio features and compare our results. For the sake of brevity, we'll only use the gradient boosted classifier as our model, as it was the strongest classifier of the our last two approaches (audio features and lyric NLP). We'll compare the results of our two feature sets and see how well our embedding vectors are able to predict a song's mood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au8E9MWPrD6B",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9VUvRKoo5YP",
        "colab_type": "code",
        "outputId": "5e15ccc2-10aa-4c8a-a76d-d64896dd41e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# load packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "from sklearn.metrics import auc, average_precision_score\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvWOOoWoqSfJ",
        "colab_type": "code",
        "outputId": "64998b96-6b34-4140-d370-99c91e4ad475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "# change field size to avoid ParseError\n",
        "#import sys\n",
        "#import csv\n",
        "\n",
        "#csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "filename = \"spotify_lyrics_clean\"\n",
        "## set engine=\"python\" to avoid ParseError\n",
        "## set error_bad_lines=False\n",
        "##df_lyrics = pd.read_csv(filename + \".csv\", index_col=0,\n",
        "##                        engine=\"python\", error_bad_lines=False)\n",
        "df_lyrics = pd.read_csv(filename + \".csv\", index_col=0)\n",
        "# make sure lyric data is type string\n",
        "#df_lyrics['lyrics_raw'] = df_lyrics['lyrics_raw'].astype(str)\n",
        "#df_lyrics['lyrics_clean'] = df_lyrics['lyrics_clean'].astype(str)\n",
        "#df_lyrics = df_lyrics.dropna().sort_index()\n",
        "#df_lyrics = df_lyrics.sort_values(\"id\").reset_index(drop=\"True\")\n",
        "df_lyrics.reset_index(drop=True, inplace=True)\n",
        "print(df_lyrics.shape)\n",
        "print(df_lyrics.dropna().shape)\n",
        "df_lyrics.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2990, 11)\n",
            "(2990, 11)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>artist</th>\n",
              "      <th>track</th>\n",
              "      <th>lyrics_raw</th>\n",
              "      <th>length_raw</th>\n",
              "      <th>lyrics_clean</th>\n",
              "      <th>length_clean</th>\n",
              "      <th>lyrics_parsed</th>\n",
              "      <th>length_parsed</th>\n",
              "      <th>lyrics_nostops</th>\n",
              "      <th>length_nostops</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7eElVgPcxDqmgGrSwVFI74</td>\n",
              "      <td>lorde</td>\n",
              "      <td>sober</td>\n",
              "      <td>[Intro]\\nNight, midnight, lose my mind\\nNight,...</td>\n",
              "      <td>2485</td>\n",
              "      <td>night midnight lose my mind night midnight lo...</td>\n",
              "      <td>2275</td>\n",
              "      <td>[' ', 'night', 'midnight', 'lose', '-PRON-', '...</td>\n",
              "      <td>494</td>\n",
              "      <td>[' ', 'night', 'midnight', 'lose', 'mind', 'ni...</td>\n",
              "      <td>187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>02oAUzv4M0ItuTDy2RT3IT</td>\n",
              "      <td>lorde</td>\n",
              "      <td>homemade dynamite</td>\n",
              "      <td>[Verse 1]\\nA couple rebel top gun pilots\\nFlyi...</td>\n",
              "      <td>1795</td>\n",
              "      <td>couple rebel top gun pilots flying with nowhe...</td>\n",
              "      <td>1621</td>\n",
              "      <td>[' ', 'couple', 'rebel', 'top', 'gun', 'pilot'...</td>\n",
              "      <td>309</td>\n",
              "      <td>[' ', 'couple', 'rebel', 'gun', 'pilot', 'fly'...</td>\n",
              "      <td>169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5q4BpnMrYEFzLO0dYODj6J</td>\n",
              "      <td>lorde</td>\n",
              "      <td>the louvre</td>\n",
              "      <td>[Verse 1]\\nWell, summer slipped us underneath ...</td>\n",
              "      <td>1824</td>\n",
              "      <td>well summer slipped us underneath her tongue ...</td>\n",
              "      <td>1665</td>\n",
              "      <td>[' ', 'well', 'summer', 'slip', '-PRON-', 'und...</td>\n",
              "      <td>334</td>\n",
              "      <td>[' ', 'summer', 'slip', 'underneath', 'tongue'...</td>\n",
              "      <td>151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6Kkt27YmFyIFrcX3QXFi2o</td>\n",
              "      <td>lorde</td>\n",
              "      <td>liability</td>\n",
              "      <td>[Intro]\\nOne, two\\n\\n[Verse 1]\\nBaby really hu...</td>\n",
              "      <td>1305</td>\n",
              "      <td>one two baby really hurt me crying in the tax...</td>\n",
              "      <td>1157</td>\n",
              "      <td>[' ', 'one', 'two', 'baby', 'really', 'hurt', ...</td>\n",
              "      <td>242</td>\n",
              "      <td>[' ', 'baby', 'hurt', 'cry', 'taxi', 'don', 'w...</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1Dp7JGFNjvg8Nk0CtMCcnr</td>\n",
              "      <td>lorde</td>\n",
              "      <td>hard feelings loveless</td>\n",
              "      <td>Hard Feelings\\n\\n(Go back and tell it)\\n\\nPlea...</td>\n",
              "      <td>2386</td>\n",
              "      <td>hard feelings go back and tell it please could...</td>\n",
              "      <td>2189</td>\n",
              "      <td>['hard', 'feeling', 'go', 'back', 'and', 'tell...</td>\n",
              "      <td>458</td>\n",
              "      <td>['hard', 'feeling', 'tell', 'tender', 'sit', '...</td>\n",
              "      <td>244</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       id  ... length_nostops\n",
              "0  7eElVgPcxDqmgGrSwVFI74  ...            187\n",
              "1  02oAUzv4M0ItuTDy2RT3IT  ...            169\n",
              "2  5q4BpnMrYEFzLO0dYODj6J  ...            151\n",
              "3  6Kkt27YmFyIFrcX3QXFi2o  ...            101\n",
              "4  1Dp7JGFNjvg8Nk0CtMCcnr  ...            244\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYp5QLousFAr",
        "colab_type": "code",
        "outputId": "0491aa23-4c17-48f2-be32-42cf21b228ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "filename = \"spotify_features\"\n",
        "Audio_features = pd.read_csv(filename + \".csv\", index_col=0)\n",
        "\n",
        "Audio_features.reset_index(drop=True, inplace=True)\n",
        "print(Audio_features.shape)\n",
        "print(Audio_features.dropna().shape)\n",
        "Audio_features.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2990, 18)\n",
            "(2990, 18)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>danceability</th>\n",
              "      <th>duration_ms</th>\n",
              "      <th>energy</th>\n",
              "      <th>explicit</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>key</th>\n",
              "      <th>liveness</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>popularity</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>tempo</th>\n",
              "      <th>time_signature</th>\n",
              "      <th>valence</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7eElVgPcxDqmgGrSwVFI74</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1730</td>\n",
              "      <td>0.796</td>\n",
              "      <td>197236</td>\n",
              "      <td>0.467</td>\n",
              "      <td>True</td>\n",
              "      <td>0.004110</td>\n",
              "      <td>6</td>\n",
              "      <td>0.1260</td>\n",
              "      <td>-10.369</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>0.1810</td>\n",
              "      <td>107.901</td>\n",
              "      <td>4</td>\n",
              "      <td>0.516</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>02oAUzv4M0ItuTDy2RT3IT</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2870</td>\n",
              "      <td>0.771</td>\n",
              "      <td>189796</td>\n",
              "      <td>0.431</td>\n",
              "      <td>True</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>10</td>\n",
              "      <td>0.0934</td>\n",
              "      <td>-5.423</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>0.0577</td>\n",
              "      <td>107.047</td>\n",
              "      <td>4</td>\n",
              "      <td>0.268</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5q4BpnMrYEFzLO0dYODj6J</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2390</td>\n",
              "      <td>0.664</td>\n",
              "      <td>271088</td>\n",
              "      <td>0.382</td>\n",
              "      <td>False</td>\n",
              "      <td>0.007490</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0943</td>\n",
              "      <td>-9.977</td>\n",
              "      <td>1</td>\n",
              "      <td>61</td>\n",
              "      <td>0.0470</td>\n",
              "      <td>123.214</td>\n",
              "      <td>4</td>\n",
              "      <td>0.126</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6Kkt27YmFyIFrcX3QXFi2o</td>\n",
              "      <td>0</td>\n",
              "      <td>0.9200</td>\n",
              "      <td>0.587</td>\n",
              "      <td>171728</td>\n",
              "      <td>0.229</td>\n",
              "      <td>False</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10</td>\n",
              "      <td>0.1000</td>\n",
              "      <td>-11.254</td>\n",
              "      <td>0</td>\n",
              "      <td>69</td>\n",
              "      <td>0.1280</td>\n",
              "      <td>75.670</td>\n",
              "      <td>4</td>\n",
              "      <td>0.379</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1Dp7JGFNjvg8Nk0CtMCcnr</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0328</td>\n",
              "      <td>0.487</td>\n",
              "      <td>367391</td>\n",
              "      <td>0.445</td>\n",
              "      <td>True</td>\n",
              "      <td>0.010400</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0778</td>\n",
              "      <td>-10.959</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>0.1130</td>\n",
              "      <td>97.031</td>\n",
              "      <td>4</td>\n",
              "      <td>0.180</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       id  target  acousticness  ...  time_signature  valence  year\n",
              "0  7eElVgPcxDqmgGrSwVFI74       0        0.1730  ...               4    0.516  2017\n",
              "1  02oAUzv4M0ItuTDy2RT3IT       0        0.2870  ...               4    0.268  2017\n",
              "2  5q4BpnMrYEFzLO0dYODj6J       0        0.2390  ...               4    0.126  2017\n",
              "3  6Kkt27YmFyIFrcX3QXFi2o       0        0.9200  ...               4    0.379  2017\n",
              "4  1Dp7JGFNjvg8Nk0CtMCcnr       0        0.0328  ...               4    0.180  2017\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLZuqY3gMIlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert (Audio_features.index == df_lyrics.index).all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwJmD13wEfK8",
        "colab_type": "code",
        "outputId": "2196786c-10f1-4bbf-de9d-f9a8576b5ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "filename = \"spotify_embeddings\"\n",
        "df_emb = pd.read_csv(filename + \".csv\", index_col=0)\n",
        "df_emb.reset_index(inplace=True)\n",
        "print(df_emb.shape)\n",
        "print(df_emb.dropna().shape)\n",
        "df_emb.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2990, 1025)\n",
            "(2990, 1025)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>984</th>\n",
              "      <th>985</th>\n",
              "      <th>986</th>\n",
              "      <th>987</th>\n",
              "      <th>988</th>\n",
              "      <th>989</th>\n",
              "      <th>990</th>\n",
              "      <th>991</th>\n",
              "      <th>992</th>\n",
              "      <th>993</th>\n",
              "      <th>994</th>\n",
              "      <th>995</th>\n",
              "      <th>996</th>\n",
              "      <th>997</th>\n",
              "      <th>998</th>\n",
              "      <th>999</th>\n",
              "      <th>1000</th>\n",
              "      <th>1001</th>\n",
              "      <th>1002</th>\n",
              "      <th>1003</th>\n",
              "      <th>1004</th>\n",
              "      <th>1005</th>\n",
              "      <th>1006</th>\n",
              "      <th>1007</th>\n",
              "      <th>1008</th>\n",
              "      <th>1009</th>\n",
              "      <th>1010</th>\n",
              "      <th>1011</th>\n",
              "      <th>1012</th>\n",
              "      <th>1013</th>\n",
              "      <th>1014</th>\n",
              "      <th>1015</th>\n",
              "      <th>1016</th>\n",
              "      <th>1017</th>\n",
              "      <th>1018</th>\n",
              "      <th>1019</th>\n",
              "      <th>1020</th>\n",
              "      <th>1021</th>\n",
              "      <th>1022</th>\n",
              "      <th>1023</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7eElVgPcxDqmgGrSwVFI74</td>\n",
              "      <td>-1.176089</td>\n",
              "      <td>-0.107737</td>\n",
              "      <td>-0.569370</td>\n",
              "      <td>-0.250905</td>\n",
              "      <td>-0.196079</td>\n",
              "      <td>0.233768</td>\n",
              "      <td>0.643306</td>\n",
              "      <td>1.164762</td>\n",
              "      <td>-0.953797</td>\n",
              "      <td>0.885341</td>\n",
              "      <td>0.233456</td>\n",
              "      <td>1.157343</td>\n",
              "      <td>-1.184159</td>\n",
              "      <td>2.234673</td>\n",
              "      <td>1.082309</td>\n",
              "      <td>1.509842</td>\n",
              "      <td>1.017656</td>\n",
              "      <td>0.275086</td>\n",
              "      <td>0.189763</td>\n",
              "      <td>0.016859</td>\n",
              "      <td>-0.937232</td>\n",
              "      <td>0.386363</td>\n",
              "      <td>-0.124789</td>\n",
              "      <td>-0.556945</td>\n",
              "      <td>-0.054143</td>\n",
              "      <td>-0.929406</td>\n",
              "      <td>-0.012605</td>\n",
              "      <td>0.002390</td>\n",
              "      <td>0.076078</td>\n",
              "      <td>1.339858</td>\n",
              "      <td>-0.044450</td>\n",
              "      <td>0.330110</td>\n",
              "      <td>0.442595</td>\n",
              "      <td>0.388205</td>\n",
              "      <td>-0.376086</td>\n",
              "      <td>0.700518</td>\n",
              "      <td>0.621598</td>\n",
              "      <td>0.748061</td>\n",
              "      <td>-0.216787</td>\n",
              "      <td>...</td>\n",
              "      <td>0.432884</td>\n",
              "      <td>2.003630</td>\n",
              "      <td>-0.852997</td>\n",
              "      <td>-0.298621</td>\n",
              "      <td>-1.123024</td>\n",
              "      <td>-1.059595</td>\n",
              "      <td>-1.441275</td>\n",
              "      <td>0.095546</td>\n",
              "      <td>2.634760</td>\n",
              "      <td>0.384049</td>\n",
              "      <td>-1.075117</td>\n",
              "      <td>-1.467201</td>\n",
              "      <td>-0.828077</td>\n",
              "      <td>-1.442020</td>\n",
              "      <td>0.099294</td>\n",
              "      <td>0.082079</td>\n",
              "      <td>1.334333</td>\n",
              "      <td>1.647908</td>\n",
              "      <td>-2.649877</td>\n",
              "      <td>0.834402</td>\n",
              "      <td>2.780420</td>\n",
              "      <td>0.148964</td>\n",
              "      <td>-0.253208</td>\n",
              "      <td>-1.001865</td>\n",
              "      <td>3.024287</td>\n",
              "      <td>-0.675499</td>\n",
              "      <td>-0.921162</td>\n",
              "      <td>-0.861756</td>\n",
              "      <td>1.483587</td>\n",
              "      <td>0.678036</td>\n",
              "      <td>0.922910</td>\n",
              "      <td>1.469242</td>\n",
              "      <td>0.220669</td>\n",
              "      <td>-0.197054</td>\n",
              "      <td>1.732071</td>\n",
              "      <td>-2.546426</td>\n",
              "      <td>1.506400</td>\n",
              "      <td>0.784334</td>\n",
              "      <td>3.248197</td>\n",
              "      <td>0.849743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>02oAUzv4M0ItuTDy2RT3IT</td>\n",
              "      <td>-0.818456</td>\n",
              "      <td>-0.335162</td>\n",
              "      <td>-0.709084</td>\n",
              "      <td>-0.453882</td>\n",
              "      <td>-0.044109</td>\n",
              "      <td>-0.442736</td>\n",
              "      <td>0.629614</td>\n",
              "      <td>1.860680</td>\n",
              "      <td>-0.421865</td>\n",
              "      <td>1.260538</td>\n",
              "      <td>0.633872</td>\n",
              "      <td>1.731610</td>\n",
              "      <td>-0.870405</td>\n",
              "      <td>1.403824</td>\n",
              "      <td>2.325687</td>\n",
              "      <td>1.376182</td>\n",
              "      <td>0.929350</td>\n",
              "      <td>1.006097</td>\n",
              "      <td>0.436990</td>\n",
              "      <td>-0.342039</td>\n",
              "      <td>-0.436087</td>\n",
              "      <td>0.256072</td>\n",
              "      <td>0.461765</td>\n",
              "      <td>-1.524332</td>\n",
              "      <td>-0.195616</td>\n",
              "      <td>-0.076263</td>\n",
              "      <td>-0.307228</td>\n",
              "      <td>-0.743518</td>\n",
              "      <td>-0.357217</td>\n",
              "      <td>0.622497</td>\n",
              "      <td>0.169596</td>\n",
              "      <td>0.113535</td>\n",
              "      <td>1.616498</td>\n",
              "      <td>0.245950</td>\n",
              "      <td>0.090101</td>\n",
              "      <td>1.020024</td>\n",
              "      <td>0.232876</td>\n",
              "      <td>0.335385</td>\n",
              "      <td>-0.841537</td>\n",
              "      <td>...</td>\n",
              "      <td>1.132815</td>\n",
              "      <td>1.599117</td>\n",
              "      <td>1.513349</td>\n",
              "      <td>0.153504</td>\n",
              "      <td>-0.541403</td>\n",
              "      <td>-0.416096</td>\n",
              "      <td>-0.143429</td>\n",
              "      <td>0.899270</td>\n",
              "      <td>2.127211</td>\n",
              "      <td>-0.448340</td>\n",
              "      <td>-0.843154</td>\n",
              "      <td>0.131879</td>\n",
              "      <td>-1.112293</td>\n",
              "      <td>-0.696016</td>\n",
              "      <td>-0.302286</td>\n",
              "      <td>-0.670839</td>\n",
              "      <td>0.732956</td>\n",
              "      <td>1.625931</td>\n",
              "      <td>-1.732838</td>\n",
              "      <td>0.208377</td>\n",
              "      <td>2.225998</td>\n",
              "      <td>0.753254</td>\n",
              "      <td>-0.109293</td>\n",
              "      <td>-1.513544</td>\n",
              "      <td>2.599236</td>\n",
              "      <td>-0.731308</td>\n",
              "      <td>-1.567038</td>\n",
              "      <td>-0.648135</td>\n",
              "      <td>0.277275</td>\n",
              "      <td>-0.293433</td>\n",
              "      <td>-1.702874</td>\n",
              "      <td>1.796735</td>\n",
              "      <td>-0.916728</td>\n",
              "      <td>0.155971</td>\n",
              "      <td>1.281588</td>\n",
              "      <td>-2.694204</td>\n",
              "      <td>1.638449</td>\n",
              "      <td>-0.664250</td>\n",
              "      <td>2.543686</td>\n",
              "      <td>1.326874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5q4BpnMrYEFzLO0dYODj6J</td>\n",
              "      <td>-0.878218</td>\n",
              "      <td>0.282790</td>\n",
              "      <td>-0.808863</td>\n",
              "      <td>-0.433924</td>\n",
              "      <td>0.407604</td>\n",
              "      <td>-0.123266</td>\n",
              "      <td>1.535059</td>\n",
              "      <td>2.379490</td>\n",
              "      <td>-0.050003</td>\n",
              "      <td>1.327469</td>\n",
              "      <td>0.951804</td>\n",
              "      <td>1.565531</td>\n",
              "      <td>-2.178823</td>\n",
              "      <td>2.941031</td>\n",
              "      <td>2.310419</td>\n",
              "      <td>0.691588</td>\n",
              "      <td>0.846726</td>\n",
              "      <td>0.990229</td>\n",
              "      <td>-0.498455</td>\n",
              "      <td>-0.957336</td>\n",
              "      <td>-0.606049</td>\n",
              "      <td>0.509263</td>\n",
              "      <td>-0.357404</td>\n",
              "      <td>-0.061091</td>\n",
              "      <td>-0.657854</td>\n",
              "      <td>0.445772</td>\n",
              "      <td>0.291457</td>\n",
              "      <td>-0.649670</td>\n",
              "      <td>-0.397943</td>\n",
              "      <td>0.907033</td>\n",
              "      <td>0.644040</td>\n",
              "      <td>1.077925</td>\n",
              "      <td>0.929998</td>\n",
              "      <td>-0.379858</td>\n",
              "      <td>-0.019066</td>\n",
              "      <td>1.889962</td>\n",
              "      <td>-0.469290</td>\n",
              "      <td>0.961969</td>\n",
              "      <td>-0.685247</td>\n",
              "      <td>...</td>\n",
              "      <td>0.403733</td>\n",
              "      <td>1.477038</td>\n",
              "      <td>-0.157647</td>\n",
              "      <td>-0.359438</td>\n",
              "      <td>-0.464443</td>\n",
              "      <td>-1.103283</td>\n",
              "      <td>-0.482082</td>\n",
              "      <td>-0.203326</td>\n",
              "      <td>2.872298</td>\n",
              "      <td>0.271690</td>\n",
              "      <td>-0.083094</td>\n",
              "      <td>-0.986870</td>\n",
              "      <td>-1.383118</td>\n",
              "      <td>-1.817282</td>\n",
              "      <td>-0.450602</td>\n",
              "      <td>0.486150</td>\n",
              "      <td>1.122400</td>\n",
              "      <td>3.002925</td>\n",
              "      <td>-2.615089</td>\n",
              "      <td>-0.075313</td>\n",
              "      <td>3.189983</td>\n",
              "      <td>0.860678</td>\n",
              "      <td>-1.037161</td>\n",
              "      <td>-0.968324</td>\n",
              "      <td>2.666715</td>\n",
              "      <td>-0.871589</td>\n",
              "      <td>-1.646715</td>\n",
              "      <td>-1.006850</td>\n",
              "      <td>1.233344</td>\n",
              "      <td>1.212906</td>\n",
              "      <td>-0.033491</td>\n",
              "      <td>1.689362</td>\n",
              "      <td>0.355652</td>\n",
              "      <td>-0.423844</td>\n",
              "      <td>1.539076</td>\n",
              "      <td>-1.454237</td>\n",
              "      <td>1.154802</td>\n",
              "      <td>0.642763</td>\n",
              "      <td>3.268840</td>\n",
              "      <td>1.259202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6Kkt27YmFyIFrcX3QXFi2o</td>\n",
              "      <td>-0.466507</td>\n",
              "      <td>0.792432</td>\n",
              "      <td>-0.494447</td>\n",
              "      <td>-0.100590</td>\n",
              "      <td>0.546727</td>\n",
              "      <td>-0.253614</td>\n",
              "      <td>0.794765</td>\n",
              "      <td>2.198832</td>\n",
              "      <td>-0.234965</td>\n",
              "      <td>0.627541</td>\n",
              "      <td>0.867905</td>\n",
              "      <td>1.829224</td>\n",
              "      <td>-1.905063</td>\n",
              "      <td>2.337999</td>\n",
              "      <td>1.501105</td>\n",
              "      <td>0.749387</td>\n",
              "      <td>1.298659</td>\n",
              "      <td>0.994754</td>\n",
              "      <td>-0.205810</td>\n",
              "      <td>-0.534288</td>\n",
              "      <td>-0.753254</td>\n",
              "      <td>0.670505</td>\n",
              "      <td>0.460462</td>\n",
              "      <td>-0.790256</td>\n",
              "      <td>-0.331815</td>\n",
              "      <td>0.215781</td>\n",
              "      <td>-0.252302</td>\n",
              "      <td>-1.027334</td>\n",
              "      <td>-0.459904</td>\n",
              "      <td>1.226175</td>\n",
              "      <td>0.722869</td>\n",
              "      <td>0.377776</td>\n",
              "      <td>0.561733</td>\n",
              "      <td>-0.383966</td>\n",
              "      <td>1.132673</td>\n",
              "      <td>1.598741</td>\n",
              "      <td>-0.868457</td>\n",
              "      <td>0.226428</td>\n",
              "      <td>-0.208647</td>\n",
              "      <td>...</td>\n",
              "      <td>0.330872</td>\n",
              "      <td>1.053059</td>\n",
              "      <td>0.763765</td>\n",
              "      <td>-0.465366</td>\n",
              "      <td>-0.088132</td>\n",
              "      <td>-0.982575</td>\n",
              "      <td>-0.554920</td>\n",
              "      <td>-0.862160</td>\n",
              "      <td>1.764676</td>\n",
              "      <td>0.451022</td>\n",
              "      <td>-0.289202</td>\n",
              "      <td>-0.921953</td>\n",
              "      <td>-1.331076</td>\n",
              "      <td>-2.114232</td>\n",
              "      <td>-0.110930</td>\n",
              "      <td>0.491957</td>\n",
              "      <td>0.588337</td>\n",
              "      <td>1.671226</td>\n",
              "      <td>-2.189916</td>\n",
              "      <td>0.884421</td>\n",
              "      <td>2.443677</td>\n",
              "      <td>0.334962</td>\n",
              "      <td>-0.400248</td>\n",
              "      <td>-1.403574</td>\n",
              "      <td>2.521202</td>\n",
              "      <td>-0.352487</td>\n",
              "      <td>-0.814183</td>\n",
              "      <td>-1.534589</td>\n",
              "      <td>0.717943</td>\n",
              "      <td>0.148626</td>\n",
              "      <td>0.648566</td>\n",
              "      <td>1.175033</td>\n",
              "      <td>0.786978</td>\n",
              "      <td>-0.146593</td>\n",
              "      <td>0.475641</td>\n",
              "      <td>-2.374639</td>\n",
              "      <td>0.846798</td>\n",
              "      <td>0.717558</td>\n",
              "      <td>3.282399</td>\n",
              "      <td>1.231623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1Dp7JGFNjvg8Nk0CtMCcnr</td>\n",
              "      <td>-0.821989</td>\n",
              "      <td>1.116456</td>\n",
              "      <td>-0.099389</td>\n",
              "      <td>0.346411</td>\n",
              "      <td>0.864321</td>\n",
              "      <td>-0.394252</td>\n",
              "      <td>0.421176</td>\n",
              "      <td>1.416335</td>\n",
              "      <td>-0.827872</td>\n",
              "      <td>1.231101</td>\n",
              "      <td>0.509105</td>\n",
              "      <td>1.946500</td>\n",
              "      <td>-1.710877</td>\n",
              "      <td>1.770404</td>\n",
              "      <td>1.396535</td>\n",
              "      <td>1.170612</td>\n",
              "      <td>0.788084</td>\n",
              "      <td>1.045131</td>\n",
              "      <td>-0.393810</td>\n",
              "      <td>-0.345053</td>\n",
              "      <td>-0.226228</td>\n",
              "      <td>1.440690</td>\n",
              "      <td>0.052912</td>\n",
              "      <td>-0.692761</td>\n",
              "      <td>-0.927573</td>\n",
              "      <td>-0.514701</td>\n",
              "      <td>-0.332411</td>\n",
              "      <td>-1.399359</td>\n",
              "      <td>-0.215262</td>\n",
              "      <td>1.182957</td>\n",
              "      <td>0.182795</td>\n",
              "      <td>0.323677</td>\n",
              "      <td>0.178181</td>\n",
              "      <td>0.203908</td>\n",
              "      <td>0.652043</td>\n",
              "      <td>1.405541</td>\n",
              "      <td>-0.505214</td>\n",
              "      <td>-0.201571</td>\n",
              "      <td>-0.538093</td>\n",
              "      <td>...</td>\n",
              "      <td>0.315548</td>\n",
              "      <td>1.845462</td>\n",
              "      <td>0.990229</td>\n",
              "      <td>-0.891164</td>\n",
              "      <td>-0.580832</td>\n",
              "      <td>-0.711713</td>\n",
              "      <td>0.014514</td>\n",
              "      <td>-0.373449</td>\n",
              "      <td>2.832333</td>\n",
              "      <td>1.111731</td>\n",
              "      <td>-0.253617</td>\n",
              "      <td>-1.314475</td>\n",
              "      <td>-0.187999</td>\n",
              "      <td>-1.473013</td>\n",
              "      <td>0.349294</td>\n",
              "      <td>1.221470</td>\n",
              "      <td>1.054354</td>\n",
              "      <td>1.784692</td>\n",
              "      <td>-1.183902</td>\n",
              "      <td>0.589098</td>\n",
              "      <td>1.978302</td>\n",
              "      <td>0.255063</td>\n",
              "      <td>-0.257339</td>\n",
              "      <td>-0.348156</td>\n",
              "      <td>2.120147</td>\n",
              "      <td>-0.839193</td>\n",
              "      <td>-1.592782</td>\n",
              "      <td>-1.119000</td>\n",
              "      <td>1.017726</td>\n",
              "      <td>0.328482</td>\n",
              "      <td>1.087067</td>\n",
              "      <td>2.510316</td>\n",
              "      <td>-0.772907</td>\n",
              "      <td>0.914355</td>\n",
              "      <td>1.909766</td>\n",
              "      <td>-2.466820</td>\n",
              "      <td>1.309808</td>\n",
              "      <td>-0.032373</td>\n",
              "      <td>3.255941</td>\n",
              "      <td>0.410061</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1025 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                       id         0         1  ...      1021      1022      1023\n",
              "0  7eElVgPcxDqmgGrSwVFI74 -1.176089 -0.107737  ...  0.784334  3.248197  0.849743\n",
              "1  02oAUzv4M0ItuTDy2RT3IT -0.818456 -0.335162  ... -0.664250  2.543686  1.326874\n",
              "2  5q4BpnMrYEFzLO0dYODj6J -0.878218  0.282790  ...  0.642763  3.268840  1.259202\n",
              "3  6Kkt27YmFyIFrcX3QXFi2o -0.466507  0.792432  ...  0.717558  3.282399  1.231623\n",
              "4  1Dp7JGFNjvg8Nk0CtMCcnr -0.821989  1.116456  ... -0.032373  3.255941  0.410061\n",
              "\n",
              "[5 rows x 1025 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpK1-yGdIdCM",
        "colab_type": "text"
      },
      "source": [
        "## 1024-Dimension Embedding Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYlOYb8zIfug",
        "colab_type": "code",
        "outputId": "dc0b9967-bd01-4d0e-b2ba-927843dfb0c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        }
      },
      "source": [
        "! pip install \"tensorflow>=1.7.0\"\n",
        "! pip install tensorflow-hub"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow>=1.7.0 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.7.0) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.7.0) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.7.0) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.7.0) (1.11.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.7.0) (0.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.7.0) (0.33.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.7.0) (1.16.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.7.0) (0.1.7)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.7.0) (1.14.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.7.0) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.7.0) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.7.0) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.7.0) (3.7.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.7.0) (0.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.7.0) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow>=1.7.0) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow>=1.7.0) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow>=1.7.0) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow>=1.7.0) (0.15.4)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.16.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nYVVOC1Ijs5",
        "colab_type": "code",
        "outputId": "8629683a-b450-42ed-f2d4-8024499e1c1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# install ELMo\n",
        "! pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/8c/72b14d20c9cbb0306939ea41109fc599302634fd5c59ccba1a659b7d0360/allennlp-0.8.4-py3-none-any.whl (5.7MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7MB 1.8MB/s \n",
            "\u001b[?25hCollecting parsimonious>=0.8.0 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 20.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.3)\n",
            "Collecting conllu==0.11 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/2c/856344d9b69baf5b374c395b4286626181a80f0c2b2f704914d18a1cea47/conllu-0.11-py2.py3-none-any.whl\n",
            "Collecting pytorch-pretrained-bert>=0.6.0 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 44.6MB/s \n",
            "\u001b[?25hCollecting awscli>=1.11.91 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/c8/31819843ec82c0fb88b8a318d21b0164f30f18ac7799dbee7b7add1b324a/awscli-1.16.195-py2.py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Collecting jsonpickle (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
            "Collecting unidecode (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.3)\n",
            "Collecting ftfy (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/86/df789c5834f15ae1ca53a8d4c1fc4788676c2e32112f6a786f2625d9c6e6/ftfy-5.5.1-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 20.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: spacy<2.2,>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.4)\n",
            "Collecting word2number>=1.1 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting overrides (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
            "Collecting numpydoc>=0.8.0 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/f3/7cfe4c616e4b9fe05540256cc9c6661c052c8a4cec2915732793b36e1843/numpydoc-0.9.1.tar.gz\n",
            "Collecting flask-cors>=3.0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.180)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\" (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/a8/adba6cd0f84ee6ab064e7f70cd03a2836cefd2e063fd565180ec13beae93/jsonnet-0.13.0.tar.gz (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
            "Collecting responses>=0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/5a/b887e89925f1de7890ef298a74438371ed4ed29b33def9e6d02dc6036fd8/responses-0.10.6-py2.py3-none-any.whl\n",
            "Collecting tensorboardX>=1.2 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 43.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.16.4)\n",
            "Collecting flaky (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/09/94d623dda1adacd51722f3e3e0f88ba08dd030ac2b2662bfb4383096340d/flaky-3.6.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from parsimonious>=0.8.0->allennlp) (1.12.0)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (2.10.1)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.15.4)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Collecting regex (from pytorch-pretrained-bert>=0.6.0->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 39.1MB/s \n",
            "\u001b[?25hCollecting rsa<=3.5.0,>=3.1.2 (from awscli>=1.11.91->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 20.3MB/s \n",
            "\u001b[?25hCollecting botocore==1.12.185 (from awscli>=1.11.91->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/5c/cb7545729b23410b51ad3f64c4c21d92d9f48f0a7ff5027c2d20bd8c298b/botocore-1.12.185-py2.py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 35.0MB/s \n",
            "\u001b[?25hCollecting colorama<=0.3.9,>=0.2.5 (from awscli>=1.11.91->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/db/c8/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf/colorama-0.3.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.2.1)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.14)\n",
            "Requirement already satisfied: PyYAML<=5.1,>=3.10; python_version != \"2.6\" in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.13)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.13.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.0.7)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.9.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (1.0.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (7.0.4)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.2.4)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (2.0.1)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (2.6.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.2.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (2.0.2)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.1.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (7.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (41.0.1)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10->flask>=1.0.2->allennlp) (1.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.9.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Building wheels for collected packages: parsimonious, word2number, overrides, numpydoc, jsonnet, regex\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/30/d1/92a39ba40f21cb70e53f8af96eb98f002a781843c065406500\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/30/ab/ae4a57b1df44fa20a531edb9601b27603da8f5336225691f3f\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "Successfully built parsimonious word2number overrides numpydoc jsonnet regex\n",
            "Installing collected packages: parsimonious, conllu, regex, pytorch-pretrained-bert, rsa, botocore, colorama, awscli, jsonpickle, unidecode, ftfy, word2number, overrides, numpydoc, flask-cors, jsonnet, responses, tensorboardX, flaky, allennlp\n",
            "  Found existing installation: rsa 4.0\n",
            "    Uninstalling rsa-4.0:\n",
            "      Successfully uninstalled rsa-4.0\n",
            "  Found existing installation: botocore 1.12.180\n",
            "    Uninstalling botocore-1.12.180:\n",
            "      Successfully uninstalled botocore-1.12.180\n",
            "Successfully installed allennlp-0.8.4 awscli-1.16.195 botocore-1.12.185 colorama-0.3.9 conllu-0.11 flaky-3.6.0 flask-cors-3.0.8 ftfy-5.5.1 jsonnet-0.13.0 jsonpickle-1.2 numpydoc-0.9.1 overrides-1.9 parsimonious-0.8.1 pytorch-pretrained-bert-0.6.2 regex-2019.6.8 responses-0.10.6 rsa-3.4.2 tensorboardX-1.8 unidecode-1.1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "rsa"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOyR5jqAIsGP",
        "colab_type": "code",
        "outputId": "f8b488d3-f758-4ea2-9d39-7cdcddb097f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "from allennlp.commands.elmo import ElmoEmbedder\n",
        "elmo = ElmoEmbedder()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  DeprecationWarning)\n",
            "100%|██████████| 336/336 [00:00<00:00, 110766.81B/s]\n",
            "100%|██████████| 374434792/374434792 [00:12<00:00, 30149296.49B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWGXcbCiIzFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lyrics = [str(x) for x in df_lyrics['lyrics_parsed']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJvLztCpI1JB",
        "colab_type": "code",
        "outputId": "8725d87a-cc00-4b26-f05e-df5d4b44f97b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%%time\n",
        "emb = elmo.embed_sentence(lyrics)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 49s, sys: 1.03 s, total: 1min 50s\n",
            "Wall time: 1min 51s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdkF1DOcI3Gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert(len(emb) == 3) # one for each layer in the ELMo output\n",
        "assert(len(emb[0]) == len(lyrics)) # the vector elements correspond with the input tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekbMdjkLI600",
        "colab_type": "code",
        "outputId": "40588b63-76a8-40e7-89c1-8b833bef2114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(emb[2].shape)\n",
        "emb[2][0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2990, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1024,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw7tpDB0JAtv",
        "colab_type": "code",
        "outputId": "d7a8edc1-01cb-447f-be18-61ff1e8f7070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "df_emb = pd.DataFrame(data=emb[2], index=df_lyrics[\"id\"])\n",
        "print(df_emb.shape)\n",
        "display(df_emb.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2990, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>984</th>\n",
              "      <th>985</th>\n",
              "      <th>986</th>\n",
              "      <th>987</th>\n",
              "      <th>988</th>\n",
              "      <th>989</th>\n",
              "      <th>990</th>\n",
              "      <th>991</th>\n",
              "      <th>992</th>\n",
              "      <th>993</th>\n",
              "      <th>994</th>\n",
              "      <th>995</th>\n",
              "      <th>996</th>\n",
              "      <th>997</th>\n",
              "      <th>998</th>\n",
              "      <th>999</th>\n",
              "      <th>1000</th>\n",
              "      <th>1001</th>\n",
              "      <th>1002</th>\n",
              "      <th>1003</th>\n",
              "      <th>1004</th>\n",
              "      <th>1005</th>\n",
              "      <th>1006</th>\n",
              "      <th>1007</th>\n",
              "      <th>1008</th>\n",
              "      <th>1009</th>\n",
              "      <th>1010</th>\n",
              "      <th>1011</th>\n",
              "      <th>1012</th>\n",
              "      <th>1013</th>\n",
              "      <th>1014</th>\n",
              "      <th>1015</th>\n",
              "      <th>1016</th>\n",
              "      <th>1017</th>\n",
              "      <th>1018</th>\n",
              "      <th>1019</th>\n",
              "      <th>1020</th>\n",
              "      <th>1021</th>\n",
              "      <th>1022</th>\n",
              "      <th>1023</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7eElVgPcxDqmgGrSwVFI74</th>\n",
              "      <td>-1.176089</td>\n",
              "      <td>-0.107737</td>\n",
              "      <td>-0.569370</td>\n",
              "      <td>-0.250905</td>\n",
              "      <td>-0.196079</td>\n",
              "      <td>0.233768</td>\n",
              "      <td>0.643306</td>\n",
              "      <td>1.164762</td>\n",
              "      <td>-0.953797</td>\n",
              "      <td>0.885341</td>\n",
              "      <td>0.233456</td>\n",
              "      <td>1.157343</td>\n",
              "      <td>-1.184159</td>\n",
              "      <td>2.234674</td>\n",
              "      <td>1.082309</td>\n",
              "      <td>1.509842</td>\n",
              "      <td>1.017656</td>\n",
              "      <td>0.275086</td>\n",
              "      <td>0.189763</td>\n",
              "      <td>0.016859</td>\n",
              "      <td>-0.937232</td>\n",
              "      <td>0.386363</td>\n",
              "      <td>-0.124789</td>\n",
              "      <td>-0.556945</td>\n",
              "      <td>-0.054143</td>\n",
              "      <td>-0.929406</td>\n",
              "      <td>-0.012605</td>\n",
              "      <td>0.002390</td>\n",
              "      <td>0.076078</td>\n",
              "      <td>1.339858</td>\n",
              "      <td>-0.044450</td>\n",
              "      <td>0.330110</td>\n",
              "      <td>0.442595</td>\n",
              "      <td>0.388205</td>\n",
              "      <td>-0.376086</td>\n",
              "      <td>0.700518</td>\n",
              "      <td>0.621598</td>\n",
              "      <td>0.748061</td>\n",
              "      <td>-0.216787</td>\n",
              "      <td>-0.314681</td>\n",
              "      <td>...</td>\n",
              "      <td>0.432884</td>\n",
              "      <td>2.003630</td>\n",
              "      <td>-0.852997</td>\n",
              "      <td>-0.298621</td>\n",
              "      <td>-1.123024</td>\n",
              "      <td>-1.059595</td>\n",
              "      <td>-1.441275</td>\n",
              "      <td>0.095546</td>\n",
              "      <td>2.634760</td>\n",
              "      <td>0.384049</td>\n",
              "      <td>-1.075117</td>\n",
              "      <td>-1.467201</td>\n",
              "      <td>-0.828077</td>\n",
              "      <td>-1.442020</td>\n",
              "      <td>0.099294</td>\n",
              "      <td>0.082079</td>\n",
              "      <td>1.334333</td>\n",
              "      <td>1.647907</td>\n",
              "      <td>-2.649877</td>\n",
              "      <td>0.834402</td>\n",
              "      <td>2.780420</td>\n",
              "      <td>0.148964</td>\n",
              "      <td>-0.253208</td>\n",
              "      <td>-1.001865</td>\n",
              "      <td>3.024287</td>\n",
              "      <td>-0.675499</td>\n",
              "      <td>-0.921162</td>\n",
              "      <td>-0.861756</td>\n",
              "      <td>1.483587</td>\n",
              "      <td>0.678036</td>\n",
              "      <td>0.922910</td>\n",
              "      <td>1.469242</td>\n",
              "      <td>0.220669</td>\n",
              "      <td>-0.197054</td>\n",
              "      <td>1.732071</td>\n",
              "      <td>-2.546426</td>\n",
              "      <td>1.506400</td>\n",
              "      <td>0.784334</td>\n",
              "      <td>3.248197</td>\n",
              "      <td>0.849743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>02oAUzv4M0ItuTDy2RT3IT</th>\n",
              "      <td>-0.818456</td>\n",
              "      <td>-0.335162</td>\n",
              "      <td>-0.709084</td>\n",
              "      <td>-0.453882</td>\n",
              "      <td>-0.044109</td>\n",
              "      <td>-0.442736</td>\n",
              "      <td>0.629614</td>\n",
              "      <td>1.860680</td>\n",
              "      <td>-0.421865</td>\n",
              "      <td>1.260538</td>\n",
              "      <td>0.633872</td>\n",
              "      <td>1.731610</td>\n",
              "      <td>-0.870405</td>\n",
              "      <td>1.403824</td>\n",
              "      <td>2.325687</td>\n",
              "      <td>1.376182</td>\n",
              "      <td>0.929350</td>\n",
              "      <td>1.006097</td>\n",
              "      <td>0.436990</td>\n",
              "      <td>-0.342039</td>\n",
              "      <td>-0.436087</td>\n",
              "      <td>0.256072</td>\n",
              "      <td>0.461765</td>\n",
              "      <td>-1.524332</td>\n",
              "      <td>-0.195616</td>\n",
              "      <td>-0.076263</td>\n",
              "      <td>-0.307228</td>\n",
              "      <td>-0.743518</td>\n",
              "      <td>-0.357217</td>\n",
              "      <td>0.622497</td>\n",
              "      <td>0.169596</td>\n",
              "      <td>0.113535</td>\n",
              "      <td>1.616498</td>\n",
              "      <td>0.245950</td>\n",
              "      <td>0.090101</td>\n",
              "      <td>1.020024</td>\n",
              "      <td>0.232876</td>\n",
              "      <td>0.335385</td>\n",
              "      <td>-0.841537</td>\n",
              "      <td>0.021720</td>\n",
              "      <td>...</td>\n",
              "      <td>1.132815</td>\n",
              "      <td>1.599117</td>\n",
              "      <td>1.513349</td>\n",
              "      <td>0.153504</td>\n",
              "      <td>-0.541403</td>\n",
              "      <td>-0.416096</td>\n",
              "      <td>-0.143429</td>\n",
              "      <td>0.899270</td>\n",
              "      <td>2.127211</td>\n",
              "      <td>-0.448340</td>\n",
              "      <td>-0.843154</td>\n",
              "      <td>0.131879</td>\n",
              "      <td>-1.112293</td>\n",
              "      <td>-0.696016</td>\n",
              "      <td>-0.302286</td>\n",
              "      <td>-0.670839</td>\n",
              "      <td>0.732956</td>\n",
              "      <td>1.625931</td>\n",
              "      <td>-1.732838</td>\n",
              "      <td>0.208377</td>\n",
              "      <td>2.225998</td>\n",
              "      <td>0.753254</td>\n",
              "      <td>-0.109293</td>\n",
              "      <td>-1.513544</td>\n",
              "      <td>2.599236</td>\n",
              "      <td>-0.731308</td>\n",
              "      <td>-1.567038</td>\n",
              "      <td>-0.648135</td>\n",
              "      <td>0.277275</td>\n",
              "      <td>-0.293433</td>\n",
              "      <td>-1.702874</td>\n",
              "      <td>1.796735</td>\n",
              "      <td>-0.916728</td>\n",
              "      <td>0.155971</td>\n",
              "      <td>1.281588</td>\n",
              "      <td>-2.694204</td>\n",
              "      <td>1.638449</td>\n",
              "      <td>-0.664250</td>\n",
              "      <td>2.543686</td>\n",
              "      <td>1.326874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5q4BpnMrYEFzLO0dYODj6J</th>\n",
              "      <td>-0.878218</td>\n",
              "      <td>0.282790</td>\n",
              "      <td>-0.808863</td>\n",
              "      <td>-0.433924</td>\n",
              "      <td>0.407604</td>\n",
              "      <td>-0.123266</td>\n",
              "      <td>1.535059</td>\n",
              "      <td>2.379490</td>\n",
              "      <td>-0.050003</td>\n",
              "      <td>1.327469</td>\n",
              "      <td>0.951804</td>\n",
              "      <td>1.565531</td>\n",
              "      <td>-2.178823</td>\n",
              "      <td>2.941031</td>\n",
              "      <td>2.310419</td>\n",
              "      <td>0.691588</td>\n",
              "      <td>0.846726</td>\n",
              "      <td>0.990229</td>\n",
              "      <td>-0.498455</td>\n",
              "      <td>-0.957336</td>\n",
              "      <td>-0.606049</td>\n",
              "      <td>0.509263</td>\n",
              "      <td>-0.357404</td>\n",
              "      <td>-0.061091</td>\n",
              "      <td>-0.657854</td>\n",
              "      <td>0.445772</td>\n",
              "      <td>0.291457</td>\n",
              "      <td>-0.649670</td>\n",
              "      <td>-0.397943</td>\n",
              "      <td>0.907033</td>\n",
              "      <td>0.644040</td>\n",
              "      <td>1.077925</td>\n",
              "      <td>0.929998</td>\n",
              "      <td>-0.379858</td>\n",
              "      <td>-0.019066</td>\n",
              "      <td>1.889962</td>\n",
              "      <td>-0.469290</td>\n",
              "      <td>0.961969</td>\n",
              "      <td>-0.685247</td>\n",
              "      <td>0.497342</td>\n",
              "      <td>...</td>\n",
              "      <td>0.403733</td>\n",
              "      <td>1.477038</td>\n",
              "      <td>-0.157647</td>\n",
              "      <td>-0.359438</td>\n",
              "      <td>-0.464443</td>\n",
              "      <td>-1.103283</td>\n",
              "      <td>-0.482082</td>\n",
              "      <td>-0.203326</td>\n",
              "      <td>2.872298</td>\n",
              "      <td>0.271690</td>\n",
              "      <td>-0.083094</td>\n",
              "      <td>-0.986870</td>\n",
              "      <td>-1.383118</td>\n",
              "      <td>-1.817282</td>\n",
              "      <td>-0.450602</td>\n",
              "      <td>0.486150</td>\n",
              "      <td>1.122401</td>\n",
              "      <td>3.002925</td>\n",
              "      <td>-2.615089</td>\n",
              "      <td>-0.075313</td>\n",
              "      <td>3.189983</td>\n",
              "      <td>0.860678</td>\n",
              "      <td>-1.037161</td>\n",
              "      <td>-0.968324</td>\n",
              "      <td>2.666715</td>\n",
              "      <td>-0.871589</td>\n",
              "      <td>-1.646716</td>\n",
              "      <td>-1.006850</td>\n",
              "      <td>1.233344</td>\n",
              "      <td>1.212906</td>\n",
              "      <td>-0.033491</td>\n",
              "      <td>1.689362</td>\n",
              "      <td>0.355652</td>\n",
              "      <td>-0.423844</td>\n",
              "      <td>1.539076</td>\n",
              "      <td>-1.454237</td>\n",
              "      <td>1.154802</td>\n",
              "      <td>0.642763</td>\n",
              "      <td>3.268840</td>\n",
              "      <td>1.259202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6Kkt27YmFyIFrcX3QXFi2o</th>\n",
              "      <td>-0.466507</td>\n",
              "      <td>0.792432</td>\n",
              "      <td>-0.494447</td>\n",
              "      <td>-0.100590</td>\n",
              "      <td>0.546727</td>\n",
              "      <td>-0.253614</td>\n",
              "      <td>0.794765</td>\n",
              "      <td>2.198832</td>\n",
              "      <td>-0.234965</td>\n",
              "      <td>0.627541</td>\n",
              "      <td>0.867905</td>\n",
              "      <td>1.829224</td>\n",
              "      <td>-1.905063</td>\n",
              "      <td>2.337999</td>\n",
              "      <td>1.501105</td>\n",
              "      <td>0.749387</td>\n",
              "      <td>1.298659</td>\n",
              "      <td>0.994754</td>\n",
              "      <td>-0.205810</td>\n",
              "      <td>-0.534288</td>\n",
              "      <td>-0.753254</td>\n",
              "      <td>0.670505</td>\n",
              "      <td>0.460462</td>\n",
              "      <td>-0.790256</td>\n",
              "      <td>-0.331815</td>\n",
              "      <td>0.215781</td>\n",
              "      <td>-0.252302</td>\n",
              "      <td>-1.027334</td>\n",
              "      <td>-0.459904</td>\n",
              "      <td>1.226176</td>\n",
              "      <td>0.722869</td>\n",
              "      <td>0.377776</td>\n",
              "      <td>0.561733</td>\n",
              "      <td>-0.383966</td>\n",
              "      <td>1.132673</td>\n",
              "      <td>1.598741</td>\n",
              "      <td>-0.868457</td>\n",
              "      <td>0.226428</td>\n",
              "      <td>-0.208647</td>\n",
              "      <td>0.573802</td>\n",
              "      <td>...</td>\n",
              "      <td>0.330872</td>\n",
              "      <td>1.053059</td>\n",
              "      <td>0.763765</td>\n",
              "      <td>-0.465366</td>\n",
              "      <td>-0.088132</td>\n",
              "      <td>-0.982575</td>\n",
              "      <td>-0.554920</td>\n",
              "      <td>-0.862160</td>\n",
              "      <td>1.764676</td>\n",
              "      <td>0.451022</td>\n",
              "      <td>-0.289202</td>\n",
              "      <td>-0.921953</td>\n",
              "      <td>-1.331076</td>\n",
              "      <td>-2.114232</td>\n",
              "      <td>-0.110930</td>\n",
              "      <td>0.491957</td>\n",
              "      <td>0.588337</td>\n",
              "      <td>1.671226</td>\n",
              "      <td>-2.189916</td>\n",
              "      <td>0.884421</td>\n",
              "      <td>2.443677</td>\n",
              "      <td>0.334962</td>\n",
              "      <td>-0.400248</td>\n",
              "      <td>-1.403574</td>\n",
              "      <td>2.521202</td>\n",
              "      <td>-0.352487</td>\n",
              "      <td>-0.814183</td>\n",
              "      <td>-1.534589</td>\n",
              "      <td>0.717942</td>\n",
              "      <td>0.148626</td>\n",
              "      <td>0.648566</td>\n",
              "      <td>1.175033</td>\n",
              "      <td>0.786978</td>\n",
              "      <td>-0.146593</td>\n",
              "      <td>0.475641</td>\n",
              "      <td>-2.374639</td>\n",
              "      <td>0.846798</td>\n",
              "      <td>0.717558</td>\n",
              "      <td>3.282399</td>\n",
              "      <td>1.231622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1Dp7JGFNjvg8Nk0CtMCcnr</th>\n",
              "      <td>-0.821989</td>\n",
              "      <td>1.116456</td>\n",
              "      <td>-0.099389</td>\n",
              "      <td>0.346411</td>\n",
              "      <td>0.864321</td>\n",
              "      <td>-0.394252</td>\n",
              "      <td>0.421176</td>\n",
              "      <td>1.416335</td>\n",
              "      <td>-0.827872</td>\n",
              "      <td>1.231101</td>\n",
              "      <td>0.509105</td>\n",
              "      <td>1.946500</td>\n",
              "      <td>-1.710877</td>\n",
              "      <td>1.770404</td>\n",
              "      <td>1.396535</td>\n",
              "      <td>1.170612</td>\n",
              "      <td>0.788084</td>\n",
              "      <td>1.045131</td>\n",
              "      <td>-0.393810</td>\n",
              "      <td>-0.345053</td>\n",
              "      <td>-0.226228</td>\n",
              "      <td>1.440690</td>\n",
              "      <td>0.052912</td>\n",
              "      <td>-0.692761</td>\n",
              "      <td>-0.927573</td>\n",
              "      <td>-0.514701</td>\n",
              "      <td>-0.332411</td>\n",
              "      <td>-1.399359</td>\n",
              "      <td>-0.215262</td>\n",
              "      <td>1.182957</td>\n",
              "      <td>0.182795</td>\n",
              "      <td>0.323677</td>\n",
              "      <td>0.178181</td>\n",
              "      <td>0.203908</td>\n",
              "      <td>0.652043</td>\n",
              "      <td>1.405541</td>\n",
              "      <td>-0.505214</td>\n",
              "      <td>-0.201571</td>\n",
              "      <td>-0.538093</td>\n",
              "      <td>0.711006</td>\n",
              "      <td>...</td>\n",
              "      <td>0.315548</td>\n",
              "      <td>1.845462</td>\n",
              "      <td>0.990229</td>\n",
              "      <td>-0.891164</td>\n",
              "      <td>-0.580832</td>\n",
              "      <td>-0.711713</td>\n",
              "      <td>0.014514</td>\n",
              "      <td>-0.373449</td>\n",
              "      <td>2.832333</td>\n",
              "      <td>1.111731</td>\n",
              "      <td>-0.253617</td>\n",
              "      <td>-1.314475</td>\n",
              "      <td>-0.187999</td>\n",
              "      <td>-1.473013</td>\n",
              "      <td>0.349294</td>\n",
              "      <td>1.221470</td>\n",
              "      <td>1.054354</td>\n",
              "      <td>1.784692</td>\n",
              "      <td>-1.183902</td>\n",
              "      <td>0.589098</td>\n",
              "      <td>1.978302</td>\n",
              "      <td>0.255063</td>\n",
              "      <td>-0.257339</td>\n",
              "      <td>-0.348156</td>\n",
              "      <td>2.120147</td>\n",
              "      <td>-0.839193</td>\n",
              "      <td>-1.592782</td>\n",
              "      <td>-1.119000</td>\n",
              "      <td>1.017726</td>\n",
              "      <td>0.328482</td>\n",
              "      <td>1.087067</td>\n",
              "      <td>2.510316</td>\n",
              "      <td>-0.772907</td>\n",
              "      <td>0.914355</td>\n",
              "      <td>1.909766</td>\n",
              "      <td>-2.466820</td>\n",
              "      <td>1.309808</td>\n",
              "      <td>-0.032373</td>\n",
              "      <td>3.255941</td>\n",
              "      <td>0.410061</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1024 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                            0         1     ...      1022      1023\n",
              "id                                          ...                    \n",
              "7eElVgPcxDqmgGrSwVFI74 -1.176089 -0.107737  ...  3.248197  0.849743\n",
              "02oAUzv4M0ItuTDy2RT3IT -0.818456 -0.335162  ...  2.543686  1.326874\n",
              "5q4BpnMrYEFzLO0dYODj6J -0.878218  0.282790  ...  3.268840  1.259202\n",
              "6Kkt27YmFyIFrcX3QXFi2o -0.466507  0.792432  ...  3.282399  1.231622\n",
              "1Dp7JGFNjvg8Nk0CtMCcnr -0.821989  1.116456  ...  3.255941  0.410061\n",
              "\n",
              "[5 rows x 1024 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_VjG5CLD6Ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "filename = \"spotify_embeddings\"\n",
        "df_emb.to_csv(filename+\".csv\")\n",
        "files.download(filename+\".csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqW2cyjyHcgc",
        "colab_type": "text"
      },
      "source": [
        "## Supervised Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G7bttS3GOYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define dataframes to store results\n",
        "model_desc = []\n",
        "model_score = pd.DataFrame()\n",
        "\n",
        "# define supervised modeling function\n",
        "def supervised_clf(model, data, target, test_size=0.25, stratify=True, cv=5):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  # split data #################################################################\n",
        "  #from sklearn.model_selection import train_test_split, cross_val_score\n",
        "  if stratify==True:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data, target, test_size=test_size, stratify=target)\n",
        "  else:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data, target, test_size=test_size)\n",
        "  \n",
        "  # fit model ##################################################################\n",
        "  model = model.fit(X_train, y_train)\n",
        "  train_score = model.score(X_train, y_train)\n",
        "  test_score = model.score(X_test, y_test)\n",
        "  print('\\nTraining score:', train_score)\n",
        "  print('\\nValidation score:', test_score)\n",
        "  \n",
        "  # cross validate #############################################################\n",
        "  cv_scores = cross_val_score(model, data, target, cv=cv)\n",
        "  cv_mean = np.round(np.mean(cv_scores), 4)\n",
        "  variance = np.round(np.var(cv_scores)*100, 4)\n",
        "  print('\\nCross validation: {} +/- {}%'.format(cv_mean, variance))\n",
        "  \n",
        "  # store results ##############################################################\n",
        "  vals = [train_score, test_score, cv_mean, variance]\n",
        "  cols = [\"train_score\", \"test_score\", \"cv_score\", \"variance\"]\n",
        "  return pd.DataFrame([vals], columns=cols)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAKlKEA7HtnG",
        "colab_type": "text"
      },
      "source": [
        "### Define Data and target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43h-NdJ96HAp",
        "colab_type": "code",
        "outputId": "911c2ee7-b2d3-4eff-81b7-5ed3d1627c99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "#df_emb.reset_index(drop=True, inplace=True)\n",
        "Data_emb = df_emb.drop(\"id\", axis=1)\n",
        "Data = pd.concat([Data_emb, Audio_features], axis=1)\n",
        "Data = Data.drop([\"target\", \"id\", \"year\"], axis=1)\n",
        "\n",
        "target = Audio_features['target']\n",
        "\n",
        "print(Data.shape)\n",
        "print(Data.dropna().shape)\n",
        "display(Data.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2990, 1039)\n",
            "(2990, 1039)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>999</th>\n",
              "      <th>1000</th>\n",
              "      <th>1001</th>\n",
              "      <th>1002</th>\n",
              "      <th>1003</th>\n",
              "      <th>1004</th>\n",
              "      <th>1005</th>\n",
              "      <th>1006</th>\n",
              "      <th>1007</th>\n",
              "      <th>1008</th>\n",
              "      <th>1009</th>\n",
              "      <th>1010</th>\n",
              "      <th>1011</th>\n",
              "      <th>1012</th>\n",
              "      <th>1013</th>\n",
              "      <th>1014</th>\n",
              "      <th>1015</th>\n",
              "      <th>1016</th>\n",
              "      <th>1017</th>\n",
              "      <th>1018</th>\n",
              "      <th>1019</th>\n",
              "      <th>1020</th>\n",
              "      <th>1021</th>\n",
              "      <th>1022</th>\n",
              "      <th>1023</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>danceability</th>\n",
              "      <th>duration_ms</th>\n",
              "      <th>energy</th>\n",
              "      <th>explicit</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>key</th>\n",
              "      <th>liveness</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>popularity</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>tempo</th>\n",
              "      <th>time_signature</th>\n",
              "      <th>valence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.176089</td>\n",
              "      <td>-0.107737</td>\n",
              "      <td>-0.569370</td>\n",
              "      <td>-0.250905</td>\n",
              "      <td>-0.196079</td>\n",
              "      <td>0.233768</td>\n",
              "      <td>0.643306</td>\n",
              "      <td>1.164762</td>\n",
              "      <td>-0.953797</td>\n",
              "      <td>0.885341</td>\n",
              "      <td>0.233456</td>\n",
              "      <td>1.157343</td>\n",
              "      <td>-1.184159</td>\n",
              "      <td>2.234673</td>\n",
              "      <td>1.082309</td>\n",
              "      <td>1.509842</td>\n",
              "      <td>1.017656</td>\n",
              "      <td>0.275086</td>\n",
              "      <td>0.189763</td>\n",
              "      <td>0.016859</td>\n",
              "      <td>-0.937232</td>\n",
              "      <td>0.386363</td>\n",
              "      <td>-0.124789</td>\n",
              "      <td>-0.556945</td>\n",
              "      <td>-0.054143</td>\n",
              "      <td>-0.929406</td>\n",
              "      <td>-0.012605</td>\n",
              "      <td>0.002390</td>\n",
              "      <td>0.076078</td>\n",
              "      <td>1.339858</td>\n",
              "      <td>-0.044450</td>\n",
              "      <td>0.330110</td>\n",
              "      <td>0.442595</td>\n",
              "      <td>0.388205</td>\n",
              "      <td>-0.376086</td>\n",
              "      <td>0.700518</td>\n",
              "      <td>0.621598</td>\n",
              "      <td>0.748061</td>\n",
              "      <td>-0.216787</td>\n",
              "      <td>-0.314681</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082079</td>\n",
              "      <td>1.334333</td>\n",
              "      <td>1.647908</td>\n",
              "      <td>-2.649877</td>\n",
              "      <td>0.834402</td>\n",
              "      <td>2.780420</td>\n",
              "      <td>0.148964</td>\n",
              "      <td>-0.253208</td>\n",
              "      <td>-1.001865</td>\n",
              "      <td>3.024287</td>\n",
              "      <td>-0.675499</td>\n",
              "      <td>-0.921162</td>\n",
              "      <td>-0.861756</td>\n",
              "      <td>1.483587</td>\n",
              "      <td>0.678036</td>\n",
              "      <td>0.922910</td>\n",
              "      <td>1.469242</td>\n",
              "      <td>0.220669</td>\n",
              "      <td>-0.197054</td>\n",
              "      <td>1.732071</td>\n",
              "      <td>-2.546426</td>\n",
              "      <td>1.506400</td>\n",
              "      <td>0.784334</td>\n",
              "      <td>3.248197</td>\n",
              "      <td>0.849743</td>\n",
              "      <td>0.1730</td>\n",
              "      <td>0.796</td>\n",
              "      <td>197236</td>\n",
              "      <td>0.467</td>\n",
              "      <td>True</td>\n",
              "      <td>0.004110</td>\n",
              "      <td>6</td>\n",
              "      <td>0.1260</td>\n",
              "      <td>-10.369</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>0.1810</td>\n",
              "      <td>107.901</td>\n",
              "      <td>4</td>\n",
              "      <td>0.516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.818456</td>\n",
              "      <td>-0.335162</td>\n",
              "      <td>-0.709084</td>\n",
              "      <td>-0.453882</td>\n",
              "      <td>-0.044109</td>\n",
              "      <td>-0.442736</td>\n",
              "      <td>0.629614</td>\n",
              "      <td>1.860680</td>\n",
              "      <td>-0.421865</td>\n",
              "      <td>1.260538</td>\n",
              "      <td>0.633872</td>\n",
              "      <td>1.731610</td>\n",
              "      <td>-0.870405</td>\n",
              "      <td>1.403824</td>\n",
              "      <td>2.325687</td>\n",
              "      <td>1.376182</td>\n",
              "      <td>0.929350</td>\n",
              "      <td>1.006097</td>\n",
              "      <td>0.436990</td>\n",
              "      <td>-0.342039</td>\n",
              "      <td>-0.436087</td>\n",
              "      <td>0.256072</td>\n",
              "      <td>0.461765</td>\n",
              "      <td>-1.524332</td>\n",
              "      <td>-0.195616</td>\n",
              "      <td>-0.076263</td>\n",
              "      <td>-0.307228</td>\n",
              "      <td>-0.743518</td>\n",
              "      <td>-0.357217</td>\n",
              "      <td>0.622497</td>\n",
              "      <td>0.169596</td>\n",
              "      <td>0.113535</td>\n",
              "      <td>1.616498</td>\n",
              "      <td>0.245950</td>\n",
              "      <td>0.090101</td>\n",
              "      <td>1.020024</td>\n",
              "      <td>0.232876</td>\n",
              "      <td>0.335385</td>\n",
              "      <td>-0.841537</td>\n",
              "      <td>0.021720</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.670839</td>\n",
              "      <td>0.732956</td>\n",
              "      <td>1.625931</td>\n",
              "      <td>-1.732838</td>\n",
              "      <td>0.208377</td>\n",
              "      <td>2.225998</td>\n",
              "      <td>0.753254</td>\n",
              "      <td>-0.109293</td>\n",
              "      <td>-1.513544</td>\n",
              "      <td>2.599236</td>\n",
              "      <td>-0.731308</td>\n",
              "      <td>-1.567038</td>\n",
              "      <td>-0.648135</td>\n",
              "      <td>0.277275</td>\n",
              "      <td>-0.293433</td>\n",
              "      <td>-1.702874</td>\n",
              "      <td>1.796735</td>\n",
              "      <td>-0.916728</td>\n",
              "      <td>0.155971</td>\n",
              "      <td>1.281588</td>\n",
              "      <td>-2.694204</td>\n",
              "      <td>1.638449</td>\n",
              "      <td>-0.664250</td>\n",
              "      <td>2.543686</td>\n",
              "      <td>1.326874</td>\n",
              "      <td>0.2870</td>\n",
              "      <td>0.771</td>\n",
              "      <td>189796</td>\n",
              "      <td>0.431</td>\n",
              "      <td>True</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>10</td>\n",
              "      <td>0.0934</td>\n",
              "      <td>-5.423</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>0.0577</td>\n",
              "      <td>107.047</td>\n",
              "      <td>4</td>\n",
              "      <td>0.268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.878218</td>\n",
              "      <td>0.282790</td>\n",
              "      <td>-0.808863</td>\n",
              "      <td>-0.433924</td>\n",
              "      <td>0.407604</td>\n",
              "      <td>-0.123266</td>\n",
              "      <td>1.535059</td>\n",
              "      <td>2.379490</td>\n",
              "      <td>-0.050003</td>\n",
              "      <td>1.327469</td>\n",
              "      <td>0.951804</td>\n",
              "      <td>1.565531</td>\n",
              "      <td>-2.178823</td>\n",
              "      <td>2.941031</td>\n",
              "      <td>2.310419</td>\n",
              "      <td>0.691588</td>\n",
              "      <td>0.846726</td>\n",
              "      <td>0.990229</td>\n",
              "      <td>-0.498455</td>\n",
              "      <td>-0.957336</td>\n",
              "      <td>-0.606049</td>\n",
              "      <td>0.509263</td>\n",
              "      <td>-0.357404</td>\n",
              "      <td>-0.061091</td>\n",
              "      <td>-0.657854</td>\n",
              "      <td>0.445772</td>\n",
              "      <td>0.291457</td>\n",
              "      <td>-0.649670</td>\n",
              "      <td>-0.397943</td>\n",
              "      <td>0.907033</td>\n",
              "      <td>0.644040</td>\n",
              "      <td>1.077925</td>\n",
              "      <td>0.929998</td>\n",
              "      <td>-0.379858</td>\n",
              "      <td>-0.019066</td>\n",
              "      <td>1.889962</td>\n",
              "      <td>-0.469290</td>\n",
              "      <td>0.961969</td>\n",
              "      <td>-0.685247</td>\n",
              "      <td>0.497342</td>\n",
              "      <td>...</td>\n",
              "      <td>0.486150</td>\n",
              "      <td>1.122400</td>\n",
              "      <td>3.002925</td>\n",
              "      <td>-2.615089</td>\n",
              "      <td>-0.075313</td>\n",
              "      <td>3.189983</td>\n",
              "      <td>0.860678</td>\n",
              "      <td>-1.037161</td>\n",
              "      <td>-0.968324</td>\n",
              "      <td>2.666715</td>\n",
              "      <td>-0.871589</td>\n",
              "      <td>-1.646715</td>\n",
              "      <td>-1.006850</td>\n",
              "      <td>1.233344</td>\n",
              "      <td>1.212906</td>\n",
              "      <td>-0.033491</td>\n",
              "      <td>1.689362</td>\n",
              "      <td>0.355652</td>\n",
              "      <td>-0.423844</td>\n",
              "      <td>1.539076</td>\n",
              "      <td>-1.454237</td>\n",
              "      <td>1.154802</td>\n",
              "      <td>0.642763</td>\n",
              "      <td>3.268840</td>\n",
              "      <td>1.259202</td>\n",
              "      <td>0.2390</td>\n",
              "      <td>0.664</td>\n",
              "      <td>271088</td>\n",
              "      <td>0.382</td>\n",
              "      <td>False</td>\n",
              "      <td>0.007490</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0943</td>\n",
              "      <td>-9.977</td>\n",
              "      <td>1</td>\n",
              "      <td>61</td>\n",
              "      <td>0.0470</td>\n",
              "      <td>123.214</td>\n",
              "      <td>4</td>\n",
              "      <td>0.126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.466507</td>\n",
              "      <td>0.792432</td>\n",
              "      <td>-0.494447</td>\n",
              "      <td>-0.100590</td>\n",
              "      <td>0.546727</td>\n",
              "      <td>-0.253614</td>\n",
              "      <td>0.794765</td>\n",
              "      <td>2.198832</td>\n",
              "      <td>-0.234965</td>\n",
              "      <td>0.627541</td>\n",
              "      <td>0.867905</td>\n",
              "      <td>1.829224</td>\n",
              "      <td>-1.905063</td>\n",
              "      <td>2.337999</td>\n",
              "      <td>1.501105</td>\n",
              "      <td>0.749387</td>\n",
              "      <td>1.298659</td>\n",
              "      <td>0.994754</td>\n",
              "      <td>-0.205810</td>\n",
              "      <td>-0.534288</td>\n",
              "      <td>-0.753254</td>\n",
              "      <td>0.670505</td>\n",
              "      <td>0.460462</td>\n",
              "      <td>-0.790256</td>\n",
              "      <td>-0.331815</td>\n",
              "      <td>0.215781</td>\n",
              "      <td>-0.252302</td>\n",
              "      <td>-1.027334</td>\n",
              "      <td>-0.459904</td>\n",
              "      <td>1.226175</td>\n",
              "      <td>0.722869</td>\n",
              "      <td>0.377776</td>\n",
              "      <td>0.561733</td>\n",
              "      <td>-0.383966</td>\n",
              "      <td>1.132673</td>\n",
              "      <td>1.598741</td>\n",
              "      <td>-0.868457</td>\n",
              "      <td>0.226428</td>\n",
              "      <td>-0.208647</td>\n",
              "      <td>0.573802</td>\n",
              "      <td>...</td>\n",
              "      <td>0.491957</td>\n",
              "      <td>0.588337</td>\n",
              "      <td>1.671226</td>\n",
              "      <td>-2.189916</td>\n",
              "      <td>0.884421</td>\n",
              "      <td>2.443677</td>\n",
              "      <td>0.334962</td>\n",
              "      <td>-0.400248</td>\n",
              "      <td>-1.403574</td>\n",
              "      <td>2.521202</td>\n",
              "      <td>-0.352487</td>\n",
              "      <td>-0.814183</td>\n",
              "      <td>-1.534589</td>\n",
              "      <td>0.717943</td>\n",
              "      <td>0.148626</td>\n",
              "      <td>0.648566</td>\n",
              "      <td>1.175033</td>\n",
              "      <td>0.786978</td>\n",
              "      <td>-0.146593</td>\n",
              "      <td>0.475641</td>\n",
              "      <td>-2.374639</td>\n",
              "      <td>0.846798</td>\n",
              "      <td>0.717558</td>\n",
              "      <td>3.282399</td>\n",
              "      <td>1.231623</td>\n",
              "      <td>0.9200</td>\n",
              "      <td>0.587</td>\n",
              "      <td>171728</td>\n",
              "      <td>0.229</td>\n",
              "      <td>False</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10</td>\n",
              "      <td>0.1000</td>\n",
              "      <td>-11.254</td>\n",
              "      <td>0</td>\n",
              "      <td>69</td>\n",
              "      <td>0.1280</td>\n",
              "      <td>75.670</td>\n",
              "      <td>4</td>\n",
              "      <td>0.379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.821989</td>\n",
              "      <td>1.116456</td>\n",
              "      <td>-0.099389</td>\n",
              "      <td>0.346411</td>\n",
              "      <td>0.864321</td>\n",
              "      <td>-0.394252</td>\n",
              "      <td>0.421176</td>\n",
              "      <td>1.416335</td>\n",
              "      <td>-0.827872</td>\n",
              "      <td>1.231101</td>\n",
              "      <td>0.509105</td>\n",
              "      <td>1.946500</td>\n",
              "      <td>-1.710877</td>\n",
              "      <td>1.770404</td>\n",
              "      <td>1.396535</td>\n",
              "      <td>1.170612</td>\n",
              "      <td>0.788084</td>\n",
              "      <td>1.045131</td>\n",
              "      <td>-0.393810</td>\n",
              "      <td>-0.345053</td>\n",
              "      <td>-0.226228</td>\n",
              "      <td>1.440690</td>\n",
              "      <td>0.052912</td>\n",
              "      <td>-0.692761</td>\n",
              "      <td>-0.927573</td>\n",
              "      <td>-0.514701</td>\n",
              "      <td>-0.332411</td>\n",
              "      <td>-1.399359</td>\n",
              "      <td>-0.215262</td>\n",
              "      <td>1.182957</td>\n",
              "      <td>0.182795</td>\n",
              "      <td>0.323677</td>\n",
              "      <td>0.178181</td>\n",
              "      <td>0.203908</td>\n",
              "      <td>0.652043</td>\n",
              "      <td>1.405541</td>\n",
              "      <td>-0.505214</td>\n",
              "      <td>-0.201571</td>\n",
              "      <td>-0.538093</td>\n",
              "      <td>0.711006</td>\n",
              "      <td>...</td>\n",
              "      <td>1.221470</td>\n",
              "      <td>1.054354</td>\n",
              "      <td>1.784692</td>\n",
              "      <td>-1.183902</td>\n",
              "      <td>0.589098</td>\n",
              "      <td>1.978302</td>\n",
              "      <td>0.255063</td>\n",
              "      <td>-0.257339</td>\n",
              "      <td>-0.348156</td>\n",
              "      <td>2.120147</td>\n",
              "      <td>-0.839193</td>\n",
              "      <td>-1.592782</td>\n",
              "      <td>-1.119000</td>\n",
              "      <td>1.017726</td>\n",
              "      <td>0.328482</td>\n",
              "      <td>1.087067</td>\n",
              "      <td>2.510316</td>\n",
              "      <td>-0.772907</td>\n",
              "      <td>0.914355</td>\n",
              "      <td>1.909766</td>\n",
              "      <td>-2.466820</td>\n",
              "      <td>1.309808</td>\n",
              "      <td>-0.032373</td>\n",
              "      <td>3.255941</td>\n",
              "      <td>0.410061</td>\n",
              "      <td>0.0328</td>\n",
              "      <td>0.487</td>\n",
              "      <td>367391</td>\n",
              "      <td>0.445</td>\n",
              "      <td>True</td>\n",
              "      <td>0.010400</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0778</td>\n",
              "      <td>-10.959</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>0.1130</td>\n",
              "      <td>97.031</td>\n",
              "      <td>4</td>\n",
              "      <td>0.180</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1039 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2  ...    tempo  time_signature  valence\n",
              "0 -1.176089 -0.107737 -0.569370  ...  107.901               4    0.516\n",
              "1 -0.818456 -0.335162 -0.709084  ...  107.047               4    0.268\n",
              "2 -0.878218  0.282790 -0.808863  ...  123.214               4    0.126\n",
              "3 -0.466507  0.792432 -0.494447  ...   75.670               4    0.379\n",
              "4 -0.821989  1.116456 -0.099389  ...   97.031               4    0.180\n",
              "\n",
              "[5 rows x 1039 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHW-nizn59-Z",
        "colab_type": "code",
        "outputId": "b8abaf78-94fa-4689-9586-505f478ea5b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# reduce data to two dimensions\n",
        "from sklearn.decomposition import PCA\n",
        "Data_2D = PCA(n_components=2).fit_transform(Data)\n",
        "\n",
        "sns.scatterplot(Data_2D[:,0], Data_2D[:,1], hue=target)\n",
        "plt.title(\"Target Variable Distribution\")\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMv4f2qrN/+0DeewAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PtTT8J9HlXT",
        "colab_type": "text"
      },
      "source": [
        "### Scale Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXUNT_UXEsRK",
        "colab_type": "code",
        "outputId": "b31993bb-d140-4767-b9a3-340528841d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "Data.drop(\"explicit\", axis=1, inplace=True)\n",
        "\n",
        "Data_raw = Data.copy()\n",
        "Data_emb_raw = Data_emb.copy()\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "Data = scaler.fit_transform(Data)\n",
        "Data = pd.DataFrame(Data, columns=Data_raw.columns)\n",
        "Data['explicit'] = Audio_features['explicit']\n",
        "\n",
        "Data_emb = scaler.fit_transform(Data_emb)\n",
        "Data_emb = pd.DataFrame(Data_emb, columns=Data_emb_raw.columns)\n",
        "\n",
        "Data.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>998</th>\n",
              "      <th>999</th>\n",
              "      <th>1000</th>\n",
              "      <th>1001</th>\n",
              "      <th>1002</th>\n",
              "      <th>1003</th>\n",
              "      <th>1004</th>\n",
              "      <th>1005</th>\n",
              "      <th>1006</th>\n",
              "      <th>1007</th>\n",
              "      <th>1008</th>\n",
              "      <th>1009</th>\n",
              "      <th>1010</th>\n",
              "      <th>1011</th>\n",
              "      <th>1012</th>\n",
              "      <th>1013</th>\n",
              "      <th>1014</th>\n",
              "      <th>1015</th>\n",
              "      <th>1016</th>\n",
              "      <th>1017</th>\n",
              "      <th>1018</th>\n",
              "      <th>1019</th>\n",
              "      <th>1020</th>\n",
              "      <th>1021</th>\n",
              "      <th>1022</th>\n",
              "      <th>1023</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>danceability</th>\n",
              "      <th>duration_ms</th>\n",
              "      <th>energy</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>key</th>\n",
              "      <th>liveness</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mode</th>\n",
              "      <th>popularity</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>tempo</th>\n",
              "      <th>time_signature</th>\n",
              "      <th>valence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "      <td>2990.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.447784</td>\n",
              "      <td>0.501948</td>\n",
              "      <td>0.564253</td>\n",
              "      <td>0.423146</td>\n",
              "      <td>0.567620</td>\n",
              "      <td>0.402528</td>\n",
              "      <td>0.501540</td>\n",
              "      <td>0.585015</td>\n",
              "      <td>0.560721</td>\n",
              "      <td>0.511011</td>\n",
              "      <td>0.506429</td>\n",
              "      <td>0.509443</td>\n",
              "      <td>0.440942</td>\n",
              "      <td>0.547279</td>\n",
              "      <td>0.596757</td>\n",
              "      <td>0.498572</td>\n",
              "      <td>0.458531</td>\n",
              "      <td>0.576142</td>\n",
              "      <td>0.587051</td>\n",
              "      <td>0.412615</td>\n",
              "      <td>0.548187</td>\n",
              "      <td>0.550772</td>\n",
              "      <td>0.618462</td>\n",
              "      <td>0.427398</td>\n",
              "      <td>0.462272</td>\n",
              "      <td>0.497970</td>\n",
              "      <td>0.527502</td>\n",
              "      <td>0.399682</td>\n",
              "      <td>0.412157</td>\n",
              "      <td>0.496677</td>\n",
              "      <td>0.494400</td>\n",
              "      <td>0.558347</td>\n",
              "      <td>0.541052</td>\n",
              "      <td>0.429396</td>\n",
              "      <td>0.488508</td>\n",
              "      <td>0.544842</td>\n",
              "      <td>0.591353</td>\n",
              "      <td>0.528764</td>\n",
              "      <td>0.368699</td>\n",
              "      <td>0.531697</td>\n",
              "      <td>...</td>\n",
              "      <td>0.490073</td>\n",
              "      <td>0.426111</td>\n",
              "      <td>0.566497</td>\n",
              "      <td>0.548438</td>\n",
              "      <td>0.503996</td>\n",
              "      <td>0.552467</td>\n",
              "      <td>0.374038</td>\n",
              "      <td>0.452642</td>\n",
              "      <td>0.457767</td>\n",
              "      <td>0.416389</td>\n",
              "      <td>0.547755</td>\n",
              "      <td>0.553965</td>\n",
              "      <td>0.394503</td>\n",
              "      <td>0.529210</td>\n",
              "      <td>0.556308</td>\n",
              "      <td>0.480971</td>\n",
              "      <td>0.432775</td>\n",
              "      <td>0.664859</td>\n",
              "      <td>0.402390</td>\n",
              "      <td>0.451705</td>\n",
              "      <td>0.403512</td>\n",
              "      <td>0.475100</td>\n",
              "      <td>0.515805</td>\n",
              "      <td>0.491892</td>\n",
              "      <td>0.569311</td>\n",
              "      <td>0.599797</td>\n",
              "      <td>0.286470</td>\n",
              "      <td>0.541853</td>\n",
              "      <td>0.304413</td>\n",
              "      <td>0.618004</td>\n",
              "      <td>0.078404</td>\n",
              "      <td>0.480207</td>\n",
              "      <td>0.164700</td>\n",
              "      <td>0.741852</td>\n",
              "      <td>0.704348</td>\n",
              "      <td>0.378416</td>\n",
              "      <td>0.072383</td>\n",
              "      <td>0.459464</td>\n",
              "      <td>0.737375</td>\n",
              "      <td>0.434673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.147768</td>\n",
              "      <td>0.145644</td>\n",
              "      <td>0.136202</td>\n",
              "      <td>0.150952</td>\n",
              "      <td>0.174615</td>\n",
              "      <td>0.138798</td>\n",
              "      <td>0.128551</td>\n",
              "      <td>0.134766</td>\n",
              "      <td>0.135424</td>\n",
              "      <td>0.157025</td>\n",
              "      <td>0.137020</td>\n",
              "      <td>0.141363</td>\n",
              "      <td>0.163968</td>\n",
              "      <td>0.148000</td>\n",
              "      <td>0.148666</td>\n",
              "      <td>0.163511</td>\n",
              "      <td>0.132606</td>\n",
              "      <td>0.130195</td>\n",
              "      <td>0.140377</td>\n",
              "      <td>0.149901</td>\n",
              "      <td>0.153733</td>\n",
              "      <td>0.128137</td>\n",
              "      <td>0.114850</td>\n",
              "      <td>0.134200</td>\n",
              "      <td>0.129205</td>\n",
              "      <td>0.142866</td>\n",
              "      <td>0.139253</td>\n",
              "      <td>0.128899</td>\n",
              "      <td>0.122169</td>\n",
              "      <td>0.136202</td>\n",
              "      <td>0.130414</td>\n",
              "      <td>0.125303</td>\n",
              "      <td>0.151312</td>\n",
              "      <td>0.123352</td>\n",
              "      <td>0.150409</td>\n",
              "      <td>0.133358</td>\n",
              "      <td>0.138841</td>\n",
              "      <td>0.144898</td>\n",
              "      <td>0.143837</td>\n",
              "      <td>0.138919</td>\n",
              "      <td>...</td>\n",
              "      <td>0.126915</td>\n",
              "      <td>0.144202</td>\n",
              "      <td>0.106244</td>\n",
              "      <td>0.146692</td>\n",
              "      <td>0.147398</td>\n",
              "      <td>0.132581</td>\n",
              "      <td>0.107810</td>\n",
              "      <td>0.135847</td>\n",
              "      <td>0.139052</td>\n",
              "      <td>0.122247</td>\n",
              "      <td>0.123818</td>\n",
              "      <td>0.149425</td>\n",
              "      <td>0.143900</td>\n",
              "      <td>0.130464</td>\n",
              "      <td>0.127126</td>\n",
              "      <td>0.113512</td>\n",
              "      <td>0.138021</td>\n",
              "      <td>0.119769</td>\n",
              "      <td>0.123248</td>\n",
              "      <td>0.132442</td>\n",
              "      <td>0.131659</td>\n",
              "      <td>0.141460</td>\n",
              "      <td>0.142246</td>\n",
              "      <td>0.155031</td>\n",
              "      <td>0.126245</td>\n",
              "      <td>0.143043</td>\n",
              "      <td>0.289535</td>\n",
              "      <td>0.145673</td>\n",
              "      <td>0.109314</td>\n",
              "      <td>0.205458</td>\n",
              "      <td>0.218966</td>\n",
              "      <td>0.327249</td>\n",
              "      <td>0.142005</td>\n",
              "      <td>0.128189</td>\n",
              "      <td>0.456412</td>\n",
              "      <td>0.257976</td>\n",
              "      <td>0.118630</td>\n",
              "      <td>0.182485</td>\n",
              "      <td>0.070155</td>\n",
              "      <td>0.248705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.339490</td>\n",
              "      <td>0.404452</td>\n",
              "      <td>0.474579</td>\n",
              "      <td>0.317677</td>\n",
              "      <td>0.449456</td>\n",
              "      <td>0.302697</td>\n",
              "      <td>0.413017</td>\n",
              "      <td>0.497577</td>\n",
              "      <td>0.473603</td>\n",
              "      <td>0.404256</td>\n",
              "      <td>0.414351</td>\n",
              "      <td>0.410845</td>\n",
              "      <td>0.329522</td>\n",
              "      <td>0.449015</td>\n",
              "      <td>0.495783</td>\n",
              "      <td>0.385876</td>\n",
              "      <td>0.368406</td>\n",
              "      <td>0.490807</td>\n",
              "      <td>0.498749</td>\n",
              "      <td>0.305475</td>\n",
              "      <td>0.445557</td>\n",
              "      <td>0.461562</td>\n",
              "      <td>0.545129</td>\n",
              "      <td>0.336615</td>\n",
              "      <td>0.375736</td>\n",
              "      <td>0.401935</td>\n",
              "      <td>0.437063</td>\n",
              "      <td>0.311005</td>\n",
              "      <td>0.329375</td>\n",
              "      <td>0.406483</td>\n",
              "      <td>0.406360</td>\n",
              "      <td>0.474999</td>\n",
              "      <td>0.439202</td>\n",
              "      <td>0.345226</td>\n",
              "      <td>0.387940</td>\n",
              "      <td>0.460966</td>\n",
              "      <td>0.499472</td>\n",
              "      <td>0.434851</td>\n",
              "      <td>0.271463</td>\n",
              "      <td>0.438184</td>\n",
              "      <td>...</td>\n",
              "      <td>0.405617</td>\n",
              "      <td>0.321604</td>\n",
              "      <td>0.498181</td>\n",
              "      <td>0.455640</td>\n",
              "      <td>0.409285</td>\n",
              "      <td>0.468497</td>\n",
              "      <td>0.302268</td>\n",
              "      <td>0.361632</td>\n",
              "      <td>0.361376</td>\n",
              "      <td>0.334978</td>\n",
              "      <td>0.466570</td>\n",
              "      <td>0.460296</td>\n",
              "      <td>0.289926</td>\n",
              "      <td>0.443992</td>\n",
              "      <td>0.477212</td>\n",
              "      <td>0.402521</td>\n",
              "      <td>0.338294</td>\n",
              "      <td>0.594153</td>\n",
              "      <td>0.318322</td>\n",
              "      <td>0.359126</td>\n",
              "      <td>0.315699</td>\n",
              "      <td>0.380038</td>\n",
              "      <td>0.423071</td>\n",
              "      <td>0.389682</td>\n",
              "      <td>0.486078</td>\n",
              "      <td>0.518823</td>\n",
              "      <td>0.036509</td>\n",
              "      <td>0.453220</td>\n",
              "      <td>0.235203</td>\n",
              "      <td>0.473857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.084051</td>\n",
              "      <td>0.673867</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.130952</td>\n",
              "      <td>0.018054</td>\n",
              "      <td>0.318799</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.237315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.445223</td>\n",
              "      <td>0.514844</td>\n",
              "      <td>0.568636</td>\n",
              "      <td>0.418186</td>\n",
              "      <td>0.578495</td>\n",
              "      <td>0.388475</td>\n",
              "      <td>0.497605</td>\n",
              "      <td>0.593961</td>\n",
              "      <td>0.564605</td>\n",
              "      <td>0.508832</td>\n",
              "      <td>0.507232</td>\n",
              "      <td>0.505047</td>\n",
              "      <td>0.437852</td>\n",
              "      <td>0.547661</td>\n",
              "      <td>0.605198</td>\n",
              "      <td>0.502824</td>\n",
              "      <td>0.456531</td>\n",
              "      <td>0.577355</td>\n",
              "      <td>0.593291</td>\n",
              "      <td>0.405084</td>\n",
              "      <td>0.546417</td>\n",
              "      <td>0.550501</td>\n",
              "      <td>0.624344</td>\n",
              "      <td>0.422961</td>\n",
              "      <td>0.461836</td>\n",
              "      <td>0.499016</td>\n",
              "      <td>0.526766</td>\n",
              "      <td>0.399580</td>\n",
              "      <td>0.407547</td>\n",
              "      <td>0.497206</td>\n",
              "      <td>0.494350</td>\n",
              "      <td>0.557250</td>\n",
              "      <td>0.545632</td>\n",
              "      <td>0.429049</td>\n",
              "      <td>0.486637</td>\n",
              "      <td>0.547214</td>\n",
              "      <td>0.599657</td>\n",
              "      <td>0.528930</td>\n",
              "      <td>0.357765</td>\n",
              "      <td>0.529380</td>\n",
              "      <td>...</td>\n",
              "      <td>0.490700</td>\n",
              "      <td>0.419600</td>\n",
              "      <td>0.562999</td>\n",
              "      <td>0.552546</td>\n",
              "      <td>0.511566</td>\n",
              "      <td>0.557665</td>\n",
              "      <td>0.373155</td>\n",
              "      <td>0.455712</td>\n",
              "      <td>0.455533</td>\n",
              "      <td>0.418440</td>\n",
              "      <td>0.550231</td>\n",
              "      <td>0.560526</td>\n",
              "      <td>0.380401</td>\n",
              "      <td>0.535262</td>\n",
              "      <td>0.563696</td>\n",
              "      <td>0.476223</td>\n",
              "      <td>0.420292</td>\n",
              "      <td>0.676485</td>\n",
              "      <td>0.396916</td>\n",
              "      <td>0.443304</td>\n",
              "      <td>0.398222</td>\n",
              "      <td>0.471847</td>\n",
              "      <td>0.519316</td>\n",
              "      <td>0.494900</td>\n",
              "      <td>0.568567</td>\n",
              "      <td>0.617113</td>\n",
              "      <td>0.176048</td>\n",
              "      <td>0.546780</td>\n",
              "      <td>0.283736</td>\n",
              "      <td>0.643671</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.108644</td>\n",
              "      <td>0.768618</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.404762</td>\n",
              "      <td>0.033501</td>\n",
              "      <td>0.455024</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.415380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.551596</td>\n",
              "      <td>0.613588</td>\n",
              "      <td>0.658799</td>\n",
              "      <td>0.519965</td>\n",
              "      <td>0.692089</td>\n",
              "      <td>0.486400</td>\n",
              "      <td>0.586324</td>\n",
              "      <td>0.682194</td>\n",
              "      <td>0.650345</td>\n",
              "      <td>0.616644</td>\n",
              "      <td>0.598596</td>\n",
              "      <td>0.603953</td>\n",
              "      <td>0.556139</td>\n",
              "      <td>0.647794</td>\n",
              "      <td>0.704015</td>\n",
              "      <td>0.612484</td>\n",
              "      <td>0.545656</td>\n",
              "      <td>0.665116</td>\n",
              "      <td>0.682045</td>\n",
              "      <td>0.510427</td>\n",
              "      <td>0.652723</td>\n",
              "      <td>0.637122</td>\n",
              "      <td>0.697511</td>\n",
              "      <td>0.514807</td>\n",
              "      <td>0.550188</td>\n",
              "      <td>0.599050</td>\n",
              "      <td>0.616510</td>\n",
              "      <td>0.483713</td>\n",
              "      <td>0.492441</td>\n",
              "      <td>0.587310</td>\n",
              "      <td>0.578647</td>\n",
              "      <td>0.641114</td>\n",
              "      <td>0.642831</td>\n",
              "      <td>0.512399</td>\n",
              "      <td>0.586526</td>\n",
              "      <td>0.630639</td>\n",
              "      <td>0.686729</td>\n",
              "      <td>0.622602</td>\n",
              "      <td>0.453650</td>\n",
              "      <td>0.621177</td>\n",
              "      <td>...</td>\n",
              "      <td>0.573040</td>\n",
              "      <td>0.518408</td>\n",
              "      <td>0.635212</td>\n",
              "      <td>0.646767</td>\n",
              "      <td>0.602711</td>\n",
              "      <td>0.643326</td>\n",
              "      <td>0.444708</td>\n",
              "      <td>0.543937</td>\n",
              "      <td>0.552339</td>\n",
              "      <td>0.500605</td>\n",
              "      <td>0.627807</td>\n",
              "      <td>0.657652</td>\n",
              "      <td>0.483843</td>\n",
              "      <td>0.615484</td>\n",
              "      <td>0.643055</td>\n",
              "      <td>0.551341</td>\n",
              "      <td>0.508255</td>\n",
              "      <td>0.747453</td>\n",
              "      <td>0.480698</td>\n",
              "      <td>0.539960</td>\n",
              "      <td>0.487801</td>\n",
              "      <td>0.565100</td>\n",
              "      <td>0.611692</td>\n",
              "      <td>0.594421</td>\n",
              "      <td>0.654571</td>\n",
              "      <td>0.700879</td>\n",
              "      <td>0.485910</td>\n",
              "      <td>0.636695</td>\n",
              "      <td>0.348560</td>\n",
              "      <td>0.788429</td>\n",
              "      <td>0.004865</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.194770</td>\n",
              "      <td>0.832345</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.069809</td>\n",
              "      <td>0.579834</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.620496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 1038 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0            1  ...  time_signature      valence\n",
              "count  2990.000000  2990.000000  ...     2990.000000  2990.000000\n",
              "mean      0.447784     0.501948  ...        0.737375     0.434673\n",
              "std       0.147768     0.145644  ...        0.070155     0.248705\n",
              "min       0.000000     0.000000  ...        0.000000     0.000000\n",
              "25%       0.339490     0.404452  ...        0.750000     0.237315\n",
              "50%       0.445223     0.514844  ...        0.750000     0.415380\n",
              "75%       0.551596     0.613588  ...        0.750000     0.620496\n",
              "max       1.000000     1.000000  ...        1.000000     1.000000\n",
              "\n",
              "[8 rows x 1038 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQEqFUzH2ad",
        "colab_type": "text"
      },
      "source": [
        "### Model Feature Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AV81xinGPVv",
        "colab_type": "code",
        "outputId": "662c6d0a-554e-4426-a9e2-ec03c0245cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "#from sklearn.ensemble import GradientBoostingClassifier\n",
        "gbc = GradientBoostingClassifier(learning_rate=0.1)\n",
        "model_desc.append(\"embeddings\")\n",
        "print(\"Features: {}\".format(model_desc[-1]))\n",
        "\n",
        "new_score = supervised_clf(model=gbc,\n",
        "                           data=Data_emb,\n",
        "                           target=target)\n",
        "model_score = model_score.append(new_score, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features: embeddings\n",
            "\n",
            "Training score: 1.0\n",
            "\n",
            "Validation score: 0.9572192513368984\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-298231c0691b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m new_score = supervised_clf(model=gbc,\n\u001b[1;32m      6\u001b[0m                            \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mData_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                            target=target)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-03fe7f8b6ff0>\u001b[0m in \u001b[0;36msupervised_clf\u001b[0;34m(model, data, target, test_size, stratify, cv)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;31m# cross validate #############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mcv_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mcv_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    387\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 231\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1544\u001b[0m         n_stages = self._fit_stages(\n\u001b[1;32m   1545\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1608\u001b[0m             raw_predictions = self._fit_stage(\n\u001b[1;32m   1609\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m                 random_state, X_idx_sorted, X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m-> 1244\u001b[0;31m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK-DL94qJOjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from sklearn.ensemble import GradientBoostingClassifier\n",
        "#gbc = GradientBoostingClassifier(learning_rate=0.1)\n",
        "model_desc.append(\"composite\")\n",
        "print(\"Features: {}\".format(model_desc[-1]))\n",
        "\n",
        "new_score = supervised_clf(model=gbc,\n",
        "                           data=Data,\n",
        "                           target=target)\n",
        "model_score = model_score.append(new_score, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy5qaqggKRbu",
        "colab_type": "text"
      },
      "source": [
        "### Assessment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsZTDZz-Zn2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_score[[\"cv_score\", \"variance\"]].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptZ3J4nwZqfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_score = pd.concat([pd.Series(model_desc, name=\"features\"), model_score], axis=1)\n",
        "model_score.sort_values(by=[\"cv_score\"], ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yp0oCC_PQPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = gbc.predict(Data)\n",
        "labels_list = [target, y_pred]\n",
        "title_list=[\"target\", \"predicted\", \"All Features: Actual vs. Predicted\"]\n",
        "data=Data_2D\n",
        "num_plots = range(0,2)\n",
        "  \n",
        "# plot target variable\n",
        "fig, axes = plt.subplots(1, len(num_plots), figsize=(10,5))\n",
        "\n",
        "for n in num_plots:\n",
        "  sns.scatterplot(data[:,0], data[:,1],\n",
        "                  hue=labels_list[n], ax=axes[n])\n",
        "  axes[n].set_xlabel(title_list[n])\n",
        "\n",
        "fig.suptitle(title_list[-1])\n",
        "plt.show;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVX8gsydPp96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "#\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=0)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        1#print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6lGvsG7PqLL",
        "colab_type": "code",
        "outputId": "fe52280c-8a14-4bfc-8693-458eb69738fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "cnf_matrix = confusion_matrix(target, y_pred)\n",
        "\n",
        "recall_metric = 100*cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1])\n",
        "print(\"Recall: {}%\".format(recall_metric))\n",
        "\n",
        "precision_metric = 100*cnf_matrix[0,0]/(cnf_matrix[0,0]+cnf_matrix[1,0])\n",
        "print(\"Precision: {}%\".format(precision_metric))\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "class_names = [0,1]\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix')\n",
        "#plt.savefig('.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall: 98.48380427291524%\n",
            "Precision: 98.57789269553976%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctRAwEJmPqaV",
        "colab_type": "code",
        "outputId": "c0766628-c599-4abe-bb00-b32c08b83321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# calculate AUC\n",
        "roc_auc = roc_auc_score(target, y_pred)\n",
        "print('ROC-AUC: %.3f' % roc_auc)\n",
        "# calculate roc curve\n",
        "fpr, tpr, thresholds = roc_curve(target, y_pred)\n",
        "\n",
        "# plot no skill\n",
        "plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "# plot the roc curve for the model\n",
        "plt.plot(fpr, tpr, marker='.', label='ROC Curve')\n",
        "plt.title('Optimized Gradient Boosted Model')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.legend()\n",
        "#plt.savefig('.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROC-AUC: 0.988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHJqc1LZPqqd",
        "colab_type": "code",
        "outputId": "bba7a4d3-5dc0-47eb-841e-7e551309b849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#from sklearn.metrics import precision_recall_curve, f1_score\n",
        "#from sklearn.metrics import auc, average_precision_score\n",
        "\n",
        "# calculate precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(target, y_pred)\n",
        "# calculate F1 score\n",
        "f1_score = f1_score(target, y_pred)\n",
        "# calculate precision-recall AUC\n",
        "precision_recall_auc = auc(recall, precision)\n",
        "# calculate average precision score\n",
        "avg_precision = average_precision_score(target, y_pred)\n",
        "print('f1=%.3f auc=%.3f ap=%.3f' % (f1_score,\n",
        "                                    precision_recall_auc,\n",
        "                                    avg_precision))\n",
        "\n",
        "# plot no skill\n",
        "plt.plot([0, 1], [0.5, 0.5], linestyle='--')\n",
        "# plot the precision-recall curve for the model\n",
        "plt.plot(recall, precision, marker='.', label='Precision-Recall Curve')\n",
        "plt.title('Optimized Gradient Boosted Model')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "#plt.savefig('.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1=0.988 auc=0.991 ap=0.983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-3XudGtN6SC",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_0sDp9vZr6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_neural_network(data, node_list=[4,1], act_list=['relu', 'sigmoid'],\n",
        "                       optimizer=\"adam\", loss=\"binary_crossentropy\",\n",
        "                       metric_list=[\"accuracy\"]):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  #  create neural network #####################################################\n",
        "  #from keras import Sequential\n",
        "  #from keras.layers import Dense\n",
        "  model = Sequential()\n",
        "  model.add(Dense(node_list[0], input_dim=data.shape[1], activation=act_list[0]))\n",
        "  model.add(Dense(node_list[0], activation=act_list[0]))\n",
        "  model.add(Dense(node_list[1], activation=act_list[1]))\n",
        "  # Compile model ##############################################################\n",
        "  model.compile(loss=loss, optimizer=optimizer, metrics=metric_list)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6BEfKLMYv7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ANN_clf(model, data, target, # data input\n",
        "                test_size=0.25, stratify=True, cv=5, # split parameters\n",
        "                batch_size=10, epochs=100, verbose=0, # training parameters\n",
        "                show_scores=True\n",
        "               ):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  # split data #################################################################\n",
        "  #from sklearn.model_selection import train_test_split\n",
        "  if stratify==True:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data, target, test_size=test_size, stratify=target)\n",
        "  else:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data, target, test_size=test_size)\n",
        "  \n",
        "  # Train network ############################################################## \n",
        "  model.fit(X_train, y_train,\n",
        "            batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
        "            validation_split=0.33)\n",
        "  \n",
        "  if show_scores==True:\n",
        "    train_eval = model.evaluate(X_train, y_train)\n",
        "    print(\"\\nTraining accuracy: {}\".format(train_eval[1]))\n",
        "\n",
        "    test_eval = model.evaluate(X_train, y_train)\n",
        "    print(\"\\nTest accuracy: {}\".format(test_eval[1]))\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "465Ev8JTYS3W",
        "colab_type": "text"
      },
      "source": [
        "### Model Feature Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM_fj_P-1YRp",
        "colab_type": "code",
        "outputId": "3a638230-e5fb-421b-a2aa-aff0be0576f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = get_neural_network(Data_emb)\n",
        "model = get_ANN_clf(model, Data_emb, target, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0710 05:01:45.595344 140083608721280 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0710 05:01:45.639106 140083608721280 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0710 05:01:45.648938 140083608721280 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0710 05:01:45.694990 140083608721280 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0710 05:01:45.718980 140083608721280 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0710 05:01:45.725475 140083608721280 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0710 05:01:45.962905 140083608721280 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1502 samples, validate on 740 samples\n",
            "Epoch 1/100\n",
            "1502/1502 [==============================] - 1s 618us/step - loss: 0.6915 - acc: 0.5686 - val_loss: 0.6804 - val_acc: 0.8459\n",
            "Epoch 2/100\n",
            "1502/1502 [==============================] - 0s 167us/step - loss: 0.6619 - acc: 0.6405 - val_loss: 0.5987 - val_acc: 0.7703\n",
            "Epoch 3/100\n",
            "1502/1502 [==============================] - 0s 159us/step - loss: 0.5482 - acc: 0.7836 - val_loss: 0.4801 - val_acc: 0.7878\n",
            "Epoch 4/100\n",
            "1502/1502 [==============================] - 0s 155us/step - loss: 0.4529 - acc: 0.8495 - val_loss: 0.3906 - val_acc: 0.8946\n",
            "Epoch 5/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.3829 - acc: 0.8795 - val_loss: 0.3259 - val_acc: 0.9068\n",
            "Epoch 6/100\n",
            "1502/1502 [==============================] - 0s 158us/step - loss: 0.3238 - acc: 0.8961 - val_loss: 0.2666 - val_acc: 0.9230\n",
            "Epoch 7/100\n",
            "1502/1502 [==============================] - 0s 150us/step - loss: 0.2565 - acc: 0.9241 - val_loss: 0.2206 - val_acc: 0.9432\n",
            "Epoch 8/100\n",
            "1502/1502 [==============================] - 0s 156us/step - loss: 0.2184 - acc: 0.9314 - val_loss: 0.2431 - val_acc: 0.9068\n",
            "Epoch 9/100\n",
            "1502/1502 [==============================] - 0s 159us/step - loss: 0.1755 - acc: 0.9527 - val_loss: 0.2289 - val_acc: 0.9108\n",
            "Epoch 10/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 0.1464 - acc: 0.9601 - val_loss: 0.1367 - val_acc: 0.9595\n",
            "Epoch 11/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.1119 - acc: 0.9707 - val_loss: 0.1157 - val_acc: 0.9676\n",
            "Epoch 12/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 0.0917 - acc: 0.9794 - val_loss: 0.0997 - val_acc: 0.9757\n",
            "Epoch 13/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0799 - acc: 0.9807 - val_loss: 0.0928 - val_acc: 0.9676\n",
            "Epoch 14/100\n",
            "1502/1502 [==============================] - 0s 150us/step - loss: 0.0754 - acc: 0.9774 - val_loss: 0.0802 - val_acc: 0.9811\n",
            "Epoch 15/100\n",
            "1502/1502 [==============================] - 0s 158us/step - loss: 0.0638 - acc: 0.9840 - val_loss: 0.0930 - val_acc: 0.9689\n",
            "Epoch 16/100\n",
            "1502/1502 [==============================] - 0s 155us/step - loss: 0.0508 - acc: 0.9907 - val_loss: 0.0676 - val_acc: 0.9811\n",
            "Epoch 17/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0537 - acc: 0.9854 - val_loss: 0.0664 - val_acc: 0.9838\n",
            "Epoch 18/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 0.0758 - acc: 0.9760 - val_loss: 0.0675 - val_acc: 0.9730\n",
            "Epoch 19/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0296 - acc: 0.9967 - val_loss: 0.0617 - val_acc: 0.9838\n",
            "Epoch 20/100\n",
            "1502/1502 [==============================] - 0s 152us/step - loss: 0.0366 - acc: 0.9933 - val_loss: 0.0588 - val_acc: 0.9770\n",
            "Epoch 21/100\n",
            "1502/1502 [==============================] - 0s 160us/step - loss: 0.0270 - acc: 0.9967 - val_loss: 0.0518 - val_acc: 0.9851\n",
            "Epoch 22/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0299 - acc: 0.9927 - val_loss: 0.0495 - val_acc: 0.9878\n",
            "Epoch 23/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 0.0197 - acc: 0.9980 - val_loss: 0.0513 - val_acc: 0.9851\n",
            "Epoch 24/100\n",
            "1502/1502 [==============================] - 0s 158us/step - loss: 0.0323 - acc: 0.9907 - val_loss: 0.0471 - val_acc: 0.9878\n",
            "Epoch 25/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0187 - acc: 0.9967 - val_loss: 0.0479 - val_acc: 0.9797\n",
            "Epoch 26/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 0.0259 - acc: 0.9940 - val_loss: 0.0517 - val_acc: 0.9757\n",
            "Epoch 27/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.0455 - val_acc: 0.9797\n",
            "Epoch 28/100\n",
            "1502/1502 [==============================] - 0s 167us/step - loss: 0.0152 - acc: 0.9980 - val_loss: 0.0499 - val_acc: 0.9757\n",
            "Epoch 29/100\n",
            "1502/1502 [==============================] - 0s 155us/step - loss: 0.0123 - acc: 0.9980 - val_loss: 0.0419 - val_acc: 0.9865\n",
            "Epoch 30/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0094 - acc: 0.9987 - val_loss: 0.0537 - val_acc: 0.9824\n",
            "Epoch 31/100\n",
            "1502/1502 [==============================] - 0s 159us/step - loss: 0.0126 - acc: 0.9967 - val_loss: 0.0487 - val_acc: 0.9797\n",
            "Epoch 32/100\n",
            "1502/1502 [==============================] - 0s 152us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.0422 - val_acc: 0.9838\n",
            "Epoch 33/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0432 - val_acc: 0.9797\n",
            "Epoch 34/100\n",
            "1502/1502 [==============================] - 0s 152us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0406 - val_acc: 0.9878\n",
            "Epoch 35/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0446 - val_acc: 0.9797\n",
            "Epoch 36/100\n",
            "1502/1502 [==============================] - 0s 156us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0444 - val_acc: 0.9811\n",
            "Epoch 37/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.0551 - val_acc: 0.9784\n",
            "Epoch 38/100\n",
            "1502/1502 [==============================] - 0s 156us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0500 - val_acc: 0.9797\n",
            "Epoch 39/100\n",
            "1502/1502 [==============================] - 0s 152us/step - loss: 0.0056 - acc: 0.9987 - val_loss: 0.0391 - val_acc: 0.9892\n",
            "Epoch 40/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0394 - val_acc: 0.9851\n",
            "Epoch 41/100\n",
            "1502/1502 [==============================] - 0s 155us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0409 - val_acc: 0.9878\n",
            "Epoch 42/100\n",
            "1502/1502 [==============================] - 0s 169us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0395 - val_acc: 0.9878\n",
            "Epoch 43/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0388 - val_acc: 0.9865\n",
            "Epoch 44/100\n",
            "1502/1502 [==============================] - 0s 151us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0392 - val_acc: 0.9851\n",
            "Epoch 45/100\n",
            "1502/1502 [==============================] - 0s 155us/step - loss: 0.0829 - acc: 0.9714 - val_loss: 0.0522 - val_acc: 0.9878\n",
            "Epoch 46/100\n",
            "1502/1502 [==============================] - 0s 152us/step - loss: 0.0355 - acc: 0.9947 - val_loss: 0.0790 - val_acc: 0.9676\n",
            "Epoch 47/100\n",
            "1502/1502 [==============================] - 0s 156us/step - loss: 0.0213 - acc: 1.0000 - val_loss: 0.0450 - val_acc: 0.9811\n",
            "Epoch 48/100\n",
            "1502/1502 [==============================] - 0s 155us/step - loss: 0.0368 - acc: 0.9887 - val_loss: 0.0492 - val_acc: 0.9824\n",
            "Epoch 49/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0186 - acc: 0.9987 - val_loss: 0.0404 - val_acc: 0.9878\n",
            "Epoch 50/100\n",
            "1502/1502 [==============================] - 0s 150us/step - loss: 0.0148 - acc: 0.9987 - val_loss: 0.0583 - val_acc: 0.9797\n",
            "Epoch 51/100\n",
            "1502/1502 [==============================] - 0s 156us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.0523 - val_acc: 0.9811\n",
            "Epoch 52/100\n",
            "1502/1502 [==============================] - 0s 157us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.0367 - val_acc: 0.9878\n",
            "Epoch 53/100\n",
            "1502/1502 [==============================] - 0s 157us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.0451 - val_acc: 0.9811\n",
            "Epoch 54/100\n",
            "1502/1502 [==============================] - 0s 155us/step - loss: 0.0126 - acc: 0.9987 - val_loss: 0.0351 - val_acc: 0.9892\n",
            "Epoch 55/100\n",
            "1502/1502 [==============================] - 0s 155us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0346 - val_acc: 0.9892\n",
            "Epoch 56/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0487 - val_acc: 0.9811\n",
            "Epoch 57/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0358 - val_acc: 0.9865\n",
            "Epoch 58/100\n",
            "1502/1502 [==============================] - 0s 155us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.0408 - val_acc: 0.9824\n",
            "Epoch 59/100\n",
            "1502/1502 [==============================] - 0s 156us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.0361 - val_acc: 0.9851\n",
            "Epoch 60/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0324 - val_acc: 0.9892\n",
            "Epoch 61/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0321 - val_acc: 0.9892\n",
            "Epoch 62/100\n",
            "1502/1502 [==============================] - 0s 158us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0312 - val_acc: 0.9892\n",
            "Epoch 63/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0643 - val_acc: 0.9730\n",
            "Epoch 64/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0310 - val_acc: 0.9892\n",
            "Epoch 65/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0303 - val_acc: 0.9919\n",
            "Epoch 66/100\n",
            "1502/1502 [==============================] - 0s 155us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0335 - val_acc: 0.9865\n",
            "Epoch 67/100\n",
            "1502/1502 [==============================] - 0s 157us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0310 - val_acc: 0.9892\n",
            "Epoch 68/100\n",
            "1502/1502 [==============================] - 0s 152us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0335 - val_acc: 0.9851\n",
            "Epoch 69/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 0.2091 - val_acc: 0.9365\n",
            "Epoch 70/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0428 - acc: 0.9820 - val_loss: 0.0318 - val_acc: 0.9905\n",
            "Epoch 71/100\n",
            "1502/1502 [==============================] - 0s 171us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0314 - val_acc: 0.9905\n",
            "Epoch 72/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0311 - val_acc: 0.9919\n",
            "Epoch 73/100\n",
            "1502/1502 [==============================] - 0s 159us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0329 - val_acc: 0.9892\n",
            "Epoch 74/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0334 - val_acc: 0.9878\n",
            "Epoch 75/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0325 - val_acc: 0.9892\n",
            "Epoch 76/100\n",
            "1502/1502 [==============================] - 0s 155us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0304 - val_acc: 0.9905\n",
            "Epoch 77/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0302 - val_acc: 0.9919\n",
            "Epoch 78/100\n",
            "1502/1502 [==============================] - 0s 158us/step - loss: 9.7164e-04 - acc: 1.0000 - val_loss: 0.0321 - val_acc: 0.9878\n",
            "Epoch 79/100\n",
            "1502/1502 [==============================] - 0s 152us/step - loss: 9.6123e-04 - acc: 1.0000 - val_loss: 0.0319 - val_acc: 0.9892\n",
            "Epoch 80/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 8.9731e-04 - acc: 1.0000 - val_loss: 0.0308 - val_acc: 0.9905\n",
            "Epoch 81/100\n",
            "1502/1502 [==============================] - 0s 155us/step - loss: 8.2954e-04 - acc: 1.0000 - val_loss: 0.0298 - val_acc: 0.9919\n",
            "Epoch 82/100\n",
            "1502/1502 [==============================] - 0s 156us/step - loss: 9.3974e-04 - acc: 1.0000 - val_loss: 0.0299 - val_acc: 0.9919\n",
            "Epoch 83/100\n",
            "1502/1502 [==============================] - 0s 157us/step - loss: 7.4977e-04 - acc: 1.0000 - val_loss: 0.0371 - val_acc: 0.9851\n",
            "Epoch 84/100\n",
            "1502/1502 [==============================] - 0s 155us/step - loss: 7.4874e-04 - acc: 1.0000 - val_loss: 0.0311 - val_acc: 0.9892\n",
            "Epoch 85/100\n",
            "1502/1502 [==============================] - 0s 161us/step - loss: 7.0071e-04 - acc: 1.0000 - val_loss: 0.0329 - val_acc: 0.9892\n",
            "Epoch 86/100\n",
            "1502/1502 [==============================] - 0s 156us/step - loss: 6.9201e-04 - acc: 1.0000 - val_loss: 0.0292 - val_acc: 0.9919\n",
            "Epoch 87/100\n",
            "1502/1502 [==============================] - 0s 158us/step - loss: 5.7708e-04 - acc: 1.0000 - val_loss: 0.0295 - val_acc: 0.9919\n",
            "Epoch 88/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 6.1353e-04 - acc: 1.0000 - val_loss: 0.0336 - val_acc: 0.9865\n",
            "Epoch 89/100\n",
            "1502/1502 [==============================] - 0s 157us/step - loss: 5.2274e-04 - acc: 1.0000 - val_loss: 0.0290 - val_acc: 0.9919\n",
            "Epoch 90/100\n",
            "1502/1502 [==============================] - 0s 147us/step - loss: 5.3622e-04 - acc: 1.0000 - val_loss: 0.0291 - val_acc: 0.9919\n",
            "Epoch 91/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 4.7099e-04 - acc: 1.0000 - val_loss: 0.0293 - val_acc: 0.9919\n",
            "Epoch 92/100\n",
            "1502/1502 [==============================] - 0s 156us/step - loss: 4.8113e-04 - acc: 1.0000 - val_loss: 0.0292 - val_acc: 0.9919\n",
            "Epoch 93/100\n",
            "1502/1502 [==============================] - 0s 151us/step - loss: 4.1244e-04 - acc: 1.0000 - val_loss: 0.0293 - val_acc: 0.9919\n",
            "Epoch 94/100\n",
            "1502/1502 [==============================] - 0s 153us/step - loss: 4.0062e-04 - acc: 1.0000 - val_loss: 0.0293 - val_acc: 0.9919\n",
            "Epoch 95/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 3.8694e-04 - acc: 1.0000 - val_loss: 0.0313 - val_acc: 0.9905\n",
            "Epoch 96/100\n",
            "1502/1502 [==============================] - 0s 152us/step - loss: 3.3330e-04 - acc: 1.0000 - val_loss: 0.0298 - val_acc: 0.9919\n",
            "Epoch 97/100\n",
            "1502/1502 [==============================] - 0s 154us/step - loss: 3.2636e-04 - acc: 1.0000 - val_loss: 0.0295 - val_acc: 0.9919\n",
            "Epoch 98/100\n",
            "1502/1502 [==============================] - 0s 155us/step - loss: 3.0344e-04 - acc: 1.0000 - val_loss: 0.0315 - val_acc: 0.9892\n",
            "Epoch 99/100\n",
            "1502/1502 [==============================] - 0s 151us/step - loss: 3.2901e-04 - acc: 1.0000 - val_loss: 0.0296 - val_acc: 0.9919\n",
            "Epoch 100/100\n",
            "1502/1502 [==============================] - 0s 155us/step - loss: 2.7821e-04 - acc: 1.0000 - val_loss: 0.0305 - val_acc: 0.9919\n",
            "2242/2242 [==============================] - 0s 22us/step\n",
            "\n",
            "Training accuracy: 0.9973238180196253\n",
            "2242/2242 [==============================] - 0s 20us/step\n",
            "\n",
            "Test accuracy: 0.9973238180196253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXUoainBWlSK",
        "colab_type": "code",
        "outputId": "48d4287c-dfbf-4e6b-e03f-d6fe0f920688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "#from sklearn.model_selection import StratifiedKFold\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "cv_scores = []\n",
        "for train, test in kfold.split(Data_emb, target):\n",
        "  # create and compile model\n",
        "\tmodel = get_neural_network(Data_emb)\n",
        "\t# Fit the model\n",
        "\tmodel = get_ANN_clf(model, Data_emb, target, verbose=0, show_scores=False)\n",
        "\t# evaluate the model\n",
        "\tscores = model.evaluate(Data_emb, target, verbose=0)\n",
        "\tprint(\"{}%: {}%\".format(model.metrics_names[1], scores[1]*100))\n",
        "\tcv_scores.append(scores[1] * 100)\n",
        "\n",
        "cv_mean = np.round(np.mean(cv_scores), 4)\n",
        "cv_var = np.round(np.std(cv_scores), 4)\n",
        "print(\"Accuracy: {}% +/- {}%\".format(cv_mean, cv_var))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc%: 99.56521739130434%\n",
            "acc%: 99.49832775919732%\n",
            "acc%: 99.63210702341138%\n",
            "acc%: 99.49832775919732%\n",
            "acc%: 99.49832775919732%\n",
            "Accuracy: 99.53846153846153% (+/- 0.0028635026453845804%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyKreatFeCzA",
        "colab_type": "code",
        "outputId": "53092d94-ca6d-4805-f5b2-7acc1b09f650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = get_neural_network(Data)\n",
        "model = get_ANN_clf(model, Data, target, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1502 samples, validate on 740 samples\n",
            "Epoch 1/100\n",
            "1502/1502 [==============================] - 2s 1ms/step - loss: 0.6934 - acc: 0.4900 - val_loss: 0.6929 - val_acc: 0.5230\n",
            "Epoch 2/100\n",
            "1502/1502 [==============================] - 1s 467us/step - loss: 0.6931 - acc: 0.5107 - val_loss: 0.6928 - val_acc: 0.5230\n",
            "Epoch 3/100\n",
            "1502/1502 [==============================] - 1s 464us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6927 - val_acc: 0.5230\n",
            "Epoch 4/100\n",
            "1502/1502 [==============================] - 1s 458us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6926 - val_acc: 0.5230\n",
            "Epoch 5/100\n",
            "1502/1502 [==============================] - 1s 454us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6926 - val_acc: 0.5230\n",
            "Epoch 6/100\n",
            "1502/1502 [==============================] - 1s 451us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6925 - val_acc: 0.5230\n",
            "Epoch 7/100\n",
            "1502/1502 [==============================] - 1s 474us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6925 - val_acc: 0.5230\n",
            "Epoch 8/100\n",
            "1502/1502 [==============================] - 1s 458us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6925 - val_acc: 0.5230\n",
            "Epoch 9/100\n",
            "1502/1502 [==============================] - 1s 458us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6925 - val_acc: 0.5230\n",
            "Epoch 10/100\n",
            "1502/1502 [==============================] - 1s 468us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 11/100\n",
            "1502/1502 [==============================] - 1s 450us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 12/100\n",
            "1502/1502 [==============================] - 1s 456us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 13/100\n",
            "1502/1502 [==============================] - 1s 455us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 14/100\n",
            "1502/1502 [==============================] - 1s 452us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 15/100\n",
            "1502/1502 [==============================] - 1s 458us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 16/100\n",
            "1502/1502 [==============================] - 1s 464us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 17/100\n",
            "1502/1502 [==============================] - 1s 470us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 18/100\n",
            "1502/1502 [==============================] - 1s 480us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 19/100\n",
            "1502/1502 [==============================] - 1s 466us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 20/100\n",
            "1502/1502 [==============================] - 1s 466us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 21/100\n",
            "1502/1502 [==============================] - 1s 470us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 22/100\n",
            "1502/1502 [==============================] - 1s 482us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 23/100\n",
            "1502/1502 [==============================] - 1s 473us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 24/100\n",
            "1502/1502 [==============================] - 1s 474us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 25/100\n",
            "1502/1502 [==============================] - 1s 481us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 26/100\n",
            "1502/1502 [==============================] - 1s 466us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 27/100\n",
            "1502/1502 [==============================] - 1s 471us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 28/100\n",
            "1502/1502 [==============================] - 1s 473us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 29/100\n",
            "1502/1502 [==============================] - 1s 473us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 30/100\n",
            "1502/1502 [==============================] - 1s 459us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 31/100\n",
            "1502/1502 [==============================] - 1s 453us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 32/100\n",
            "1502/1502 [==============================] - 1s 463us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 33/100\n",
            "1502/1502 [==============================] - 1s 461us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 34/100\n",
            "1502/1502 [==============================] - 1s 452us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 35/100\n",
            "1502/1502 [==============================] - 1s 460us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 36/100\n",
            "1502/1502 [==============================] - 1s 470us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 37/100\n",
            "1502/1502 [==============================] - 1s 454us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 38/100\n",
            "1502/1502 [==============================] - 1s 451us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 39/100\n",
            "1502/1502 [==============================] - 1s 451us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 40/100\n",
            "1502/1502 [==============================] - 1s 451us/step - loss: 0.6929 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 41/100\n",
            "1502/1502 [==============================] - 1s 457us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 42/100\n",
            "1502/1502 [==============================] - 1s 460us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 43/100\n",
            "1502/1502 [==============================] - 1s 452us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 44/100\n",
            "1502/1502 [==============================] - 1s 453us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 45/100\n",
            "1502/1502 [==============================] - 1s 460us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 46/100\n",
            "1502/1502 [==============================] - 1s 461us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 47/100\n",
            "1502/1502 [==============================] - 1s 472us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 48/100\n",
            "1502/1502 [==============================] - 1s 456us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 49/100\n",
            "1502/1502 [==============================] - 1s 453us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 50/100\n",
            "1502/1502 [==============================] - 1s 453us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 51/100\n",
            "1502/1502 [==============================] - 1s 476us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 52/100\n",
            "1502/1502 [==============================] - 1s 459us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 53/100\n",
            "1502/1502 [==============================] - 1s 463us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 54/100\n",
            "1502/1502 [==============================] - 1s 461us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 55/100\n",
            "1502/1502 [==============================] - 1s 456us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 56/100\n",
            "1502/1502 [==============================] - 1s 458us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 57/100\n",
            "1502/1502 [==============================] - 1s 463us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 58/100\n",
            "1502/1502 [==============================] - 1s 456us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 59/100\n",
            "1502/1502 [==============================] - 1s 452us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 60/100\n",
            "1502/1502 [==============================] - 1s 454us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 61/100\n",
            "1502/1502 [==============================] - 1s 453us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 62/100\n",
            "1502/1502 [==============================] - 1s 462us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 63/100\n",
            "1502/1502 [==============================] - 1s 457us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 64/100\n",
            "1502/1502 [==============================] - 1s 455us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 65/100\n",
            "1502/1502 [==============================] - 1s 455us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 66/100\n",
            "1502/1502 [==============================] - 1s 468us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 67/100\n",
            "1502/1502 [==============================] - 1s 454us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 68/100\n",
            "1502/1502 [==============================] - 1s 458us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 69/100\n",
            "1502/1502 [==============================] - 1s 456us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 70/100\n",
            "1502/1502 [==============================] - 1s 467us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 71/100\n",
            "1502/1502 [==============================] - 1s 461us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 72/100\n",
            "1502/1502 [==============================] - 1s 461us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 73/100\n",
            "1502/1502 [==============================] - 1s 456us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 74/100\n",
            "1502/1502 [==============================] - 1s 457us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 75/100\n",
            "1502/1502 [==============================] - 1s 456us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 76/100\n",
            "1502/1502 [==============================] - 1s 457us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 77/100\n",
            "1502/1502 [==============================] - 1s 471us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 78/100\n",
            "1502/1502 [==============================] - 1s 457us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 79/100\n",
            "1502/1502 [==============================] - 1s 459us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 80/100\n",
            "1502/1502 [==============================] - 1s 455us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 81/100\n",
            "1502/1502 [==============================] - 1s 468us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 82/100\n",
            "1502/1502 [==============================] - 1s 451us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 83/100\n",
            "1502/1502 [==============================] - 1s 454us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 84/100\n",
            "1502/1502 [==============================] - 1s 450us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 85/100\n",
            "1502/1502 [==============================] - 1s 459us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 86/100\n",
            "1502/1502 [==============================] - 1s 459us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 87/100\n",
            "1502/1502 [==============================] - 1s 460us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 88/100\n",
            "1502/1502 [==============================] - 1s 467us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 89/100\n",
            "1502/1502 [==============================] - 1s 458us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 90/100\n",
            "1502/1502 [==============================] - 1s 454us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 91/100\n",
            "1502/1502 [==============================] - 1s 458us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 92/100\n",
            "1502/1502 [==============================] - 1s 460us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 93/100\n",
            "1502/1502 [==============================] - 1s 458us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 94/100\n",
            "1502/1502 [==============================] - 1s 455us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 95/100\n",
            "1502/1502 [==============================] - 1s 465us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 96/100\n",
            "1502/1502 [==============================] - 1s 456us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6923 - val_acc: 0.5230\n",
            "Epoch 97/100\n",
            "1502/1502 [==============================] - 1s 455us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 98/100\n",
            "1502/1502 [==============================] - 1s 458us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 99/100\n",
            "1502/1502 [==============================] - 1s 460us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "Epoch 100/100\n",
            "1502/1502 [==============================] - 1s 462us/step - loss: 0.6930 - acc: 0.5107 - val_loss: 0.6924 - val_acc: 0.5230\n",
            "2242/2242 [==============================] - 0s 123us/step\n",
            "\n",
            "Training accuracy: 0.5147190008920607\n",
            "2242/2242 [==============================] - 0s 122us/step\n",
            "\n",
            "Test accuracy: 0.5147190008920607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Te_VrMR5rI",
        "colab_type": "code",
        "outputId": "6f4bd3b7-e9d6-4896-8d37-65b6e222ab93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "#from sklearn.model_selection import StratifiedKFold\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "cv_scores = []\n",
        "for train, test in kfold.split(Data, target):\n",
        "  # create and compile model\n",
        "\tmodel = get_neural_network(Data)\n",
        "\t# Fit the model\n",
        "\tmodel = get_ANN_clf(model, Data, target, verbose=0, show_scores=False)\n",
        "\t# evaluate the model\n",
        "\tscores = model.evaluate(Data, target, verbose=0)\n",
        "\tprint(\"{}%: {}%\".format(model.metrics_names[1], scores[1]*100))\n",
        "\tcv_scores.append(scores[1] * 100)\n",
        "\n",
        "cv_mean = np.round(np.mean(cv_scores), 4)\n",
        "cv_var = np.round(np.std(cv_scores)*100, 4)\n",
        "print(\"Accuracy: {}% +/- {}%\".format(cv_mean, cv_var))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc%: 99.56521739130434%\n",
            "acc%: 99.39799331103679%\n",
            "acc%: 99.69899665551839%\n",
            "acc%: 99.63210702341138%\n",
            "acc%: 51.47157190635452%\n",
            "Accuracy: 89.9532% +/- 37021.8499%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK3t80pPRzTO",
        "colab_type": "text"
      },
      "source": [
        "### Assessment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIA4qq07R3WY",
        "colab_type": "code",
        "outputId": "0302e292-c309-4fb2-a03b-bcc26baf2694",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "model = get_neural_network(Data_emb)\n",
        "model = get_ANN_clf(model, Data_emb, target, verbose=0)\n",
        "y_pred = model.predict(Data_emb)\n",
        "y_pred =(y_pred>0.5).ravel()\n",
        "#y_pred = np.where(y_pred==True, 1, 0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2242/2242 [==============================] - 0s 37us/step\n",
            "\n",
            "Training accuracy: 0.9946476360392507\n",
            "2242/2242 [==============================] - 0s 36us/step\n",
            "\n",
            "Test accuracy: 0.9946476360392507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeCPe3j_Siuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_list = [target, y_pred]\n",
        "title_list=[\"target\", \"predicted\", \"All Features: Actual vs. Predicted\"]\n",
        "data=Data_2D\n",
        "num_plots = range(0,2)\n",
        "  \n",
        "# plot target variable\n",
        "fig, axes = plt.subplots(1, len(num_plots), figsize=(10,5))\n",
        "\n",
        "for n in num_plots:\n",
        "  sns.scatterplot(data[:,0], data[:,1],\n",
        "                  hue=labels_list[n], ax=axes[n])\n",
        "  axes[n].set_xlabel(title_list[n])\n",
        "\n",
        "fig.suptitle(title_list[-1])\n",
        "plt.show;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c3510e5b-706e-44e8-c852-9e0427b11890",
        "id": "2P2pR3aUSt7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "cnf_matrix = confusion_matrix(target, y_pred)\n",
        "\n",
        "recall_metric = 100*cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1])\n",
        "print(\"Recall: {}%\".format(recall_metric))\n",
        "\n",
        "precision_metric = 100*cnf_matrix[0,0]/(cnf_matrix[0,0]+cnf_matrix[1,0])\n",
        "print(\"Precision: {}%\".format(precision_metric))\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "class_names = [0,1]\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix')\n",
        "#plt.savefig('.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall: 98.96623018607856%\n",
            "Precision: 99.02786779001944%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7a71d0a0-1408-496b-a82d-28891f229e2e",
        "id": "bjJPTNkISt8P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# calculate AUC\n",
        "roc_auc = roc_auc_score(target, y_pred)\n",
        "print('ROC-AUC: %.3f' % roc_auc)\n",
        "# calculate roc curve\n",
        "fpr, tpr, thresholds = roc_curve(target, y_pred)\n",
        "\n",
        "# plot no skill\n",
        "plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "# plot the roc curve for the model\n",
        "plt.plot(fpr, tpr, marker='.', label='ROC Curve')\n",
        "plt.title('Optimized Gradient Boosted Model')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.legend()\n",
        "#plt.savefig('.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROC-AUC: 0.991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "513f2914-6698-4741-b455-fcab7ba11c2c",
        "id": "Dh7_xAyISt9E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "from sklearn.metrics import auc, average_precision_score\n",
        "\n",
        "# calculate precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(target, y_pred)\n",
        "# calculate F1 score\n",
        "f1_score = f1_score(target, y_pred)\n",
        "# calculate precision-recall AUC\n",
        "precision_recall_auc = auc(recall, precision)\n",
        "# calculate average precision score\n",
        "avg_precision = average_precision_score(target, y_pred)\n",
        "print('f1=%.3f auc=%.3f ap=%.3f' % (f1_score,\n",
        "                                    precision_recall_auc,\n",
        "                                    avg_precision))\n",
        "\n",
        "# plot no skill\n",
        "plt.plot([0, 1], [0.5, 0.5], linestyle='--')\n",
        "# plot the precision-recall curve for the model\n",
        "plt.plot(recall, precision, marker='.', label='Precision-Recall Curve')\n",
        "plt.title('Optimized Gradient Boosted Model')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "#plt.savefig('.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1=0.991 auc=0.994 ap=0.987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj1o0uJSZzCI",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UpgqBwWaZGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}